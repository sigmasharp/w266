{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1, Part 2: RNN Language Model\n",
    "**(45 points total)**\n",
    "\n",
    "In this part of the assignment, you'll implement a recurrent neural network language model. This class of models represents the cutting edge in language modeling, and what you implement will be include many of the same features.\n",
    "\n",
    "As a reference, you may want to review the following papers:\n",
    "\n",
    "- [Recurrent neural network based language model](http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf) (Mikolov, et al. 2010)\n",
    "- [Exploring the Limits of Language Modeling](http://arxiv.org/pdf/1602.02410.pdf) (Jozefowicz, et al. 2016)\n",
    "\n",
    "You'll be writing a fair amount of TensorFlow code, so you may want to review the [Week 1 TensorFlow tutorial](../week1/TensorFlow%20Tutorial.ipynb) or the [Week 4 notebook](../week4/Neural%20Probabilistic%20Language%20Model.ipynb) for review. For documentation on specific functions, consult the [TensorFlow API reference](https://www.tensorflow.org/versions/r0.10/api_docs/python/index.html).\n",
    "\n",
    "Be sure you're using a recent installation of **TensorFlow version 0.9.0 or higher**.\n",
    "\n",
    "#### Note on Indentation\n",
    "\n",
    "This notebook (as well as rnnlm.py) uses 2-space indentation, instead of the 4 spaces that is Python's default. Why? It makes lines shorter, which is handy if you have a lot of nested scopes. Some people will yell at you for doing this, but the instructors don't care either way.\n",
    "\n",
    "You can follow [these instructions](http://jupyter-notebook.readthedocs.io/en/latest/frontend_config.html#example-changing-the-notebook-s-default-indentation) to configure Jupyter to be cool about this. In Chrome, you can open a JavaScript console by going to Menu -> More Tools -> Developer Tools and clicking the \"Console\" tab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 Overview\n",
    "\n",
    "- **(a)** RNNLM Inputs and Parameters (written questions)\n",
    "- **(b)** Implementing the RNNLM\n",
    "- **(c)** Training your RNNLM\n",
    "- **(d)** Sampling Sentences\n",
    "- **(e)** Linguistic Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> l\n",
      "\n",
      "Packages:\n",
      "  [ ] abc................. Australian Broadcasting Commission 2006\n",
      "  [ ] alpino.............. Alpino Dutch Treebank\n",
      "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
      "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
      "  [ ] basque_grammars..... Grammars for Basque\n",
      "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
      "                           Extraction Systems in Biology)\n",
      "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
      "  [ ] book_grammars....... Grammars from NLTK Book\n",
      "  [ ] brown............... Brown Corpus\n",
      "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
      "  [ ] cess_cat............ CESS-CAT Treebank\n",
      "  [ ] cess_esp............ CESS-ESP Treebank\n",
      "  [ ] chat80.............. Chat-80 Data Files\n",
      "  [ ] city_database....... City Database\n",
      "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
      "  [ ] comparative_sentences Comparative Sentence Dataset\n",
      "  [ ] comtrans............ ComTrans Corpus Sample\n",
      "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
      "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
      "Hit Enter to continue: d all\n",
      "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
      "                           and Basque Subset)\n",
      "  [ ] crubadan............ Crubadan Corpus\n",
      "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
      "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
      "                           Corpus\n",
      "  [ ] floresta............ Portuguese Treebank\n",
      "  [ ] framenet_v15........ FrameNet 1.5\n",
      "  [ ] gazetteers.......... Gazeteer Lists\n",
      "  [ ] genesis............. Genesis Corpus\n",
      "  [ ] gutenberg........... Project Gutenberg Selections\n",
      "  [ ] hmm_treebank_pos_tagger Treebank Part of Speech Tagger (HMM)\n",
      "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
      "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
      "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
      "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
      "                           ChaSen format)\n",
      "  [ ] kimmo............... PC-KIMMO Data Files\n",
      "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
      "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
      "                           for parser comparison\n",
      "Hit Enter to continue: \n",
      "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
      "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
      "                           part-of-speech tags\n",
      "  [ ] machado............. Machado de Assis -- Obra Completa\n",
      "  [ ] masc_tagged......... MASC Tagged Corpus\n",
      "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
      "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
      "  [ ] moses_sample........ Moses Sample Models\n",
      "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
      "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
      "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
      "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
      "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
      "  [ ] nps_chat............ NPS Chat\n",
      "  [ ] omw................. Open Multilingual Wordnet\n",
      "  [ ] opinion_lexicon..... Opinion Lexicon\n",
      "  [ ] panlex_lite......... PanLex Lite Corpus\n",
      "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
      "  [ ] paradigms........... Paradigm Corpus\n",
      "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
      "                           Evaluation Shared Task\n",
      "Hit Enter to continue: \n",
      "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
      "                           character properties in Perl\n",
      "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
      "  [ ] pl196x.............. Polish language of the XX century sixties\n",
      "  [ ] porter_test......... Porter Stemmer Test Files\n",
      "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
      "  [ ] problem_reports..... Problem Report Corpus\n",
      "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
      "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
      "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
      "  [ ] pros_cons........... Pros and Cons\n",
      "  [ ] ptb................. Penn Treebank\n",
      "  [ ] punkt............... Punkt Tokenizer Models\n",
      "  [ ] qc.................. Experimental Data for Question Classification\n",
      "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
      "                           version\n",
      "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
      "                           Portuguesa)\n",
      "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
      "  [ ] sample_grammars..... Sample Grammars\n",
      "  [ ] semcor.............. SemCor 3.0\n",
      "Hit Enter to continue: \n",
      "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
      "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
      "  [ ] sentiwordnet........ SentiWordNet\n",
      "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
      "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
      "  [ ] smultron............ SMULTRON Corpus Sample\n",
      "  [ ] snowball_data....... Snowball Data\n",
      "  [ ] spanish_grammars.... Grammars for Spanish\n",
      "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
      "  [ ] stopwords........... Stopwords Corpus\n",
      "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
      "  [ ] swadesh............. Swadesh Wordlists\n",
      "  [ ] switchboard......... Switchboard Corpus Sample\n",
      "  [ ] tagsets............. Help on Tagsets\n",
      "  [ ] timit............... TIMIT Corpus Sample\n",
      "  [ ] toolbox............. Toolbox Sample Files\n",
      "  [ ] treebank............ Penn Treebank Sample\n",
      "  [ ] twitter_samples..... Twitter Samples\n",
      "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
      "                           (Unicode Version)\n",
      "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
      "Hit Enter to continue: \n",
      "  [ ] unicode_samples..... Unicode Samples\n",
      "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
      "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
      "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
      "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
      "  [ ] webtext............. Web Text Corpus\n",
      "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
      "  [ ] word2vec_sample..... Word2Vec Sample\n",
      "  [ ] wordnet............. WordNet\n",
      "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
      "  [ ] words............... Word Lists\n",
      "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
      "                           English Prose\n",
      "\n",
      "Collections:\n",
      "  [ ] all-corpora......... All the corpora\n",
      "  [ ] all................. All packages\n",
      "  [ ] book................ Everything used in the NLTK Book\n",
      "\n",
      "([*] marks installed packages)\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> d all-corpora\n",
      "    Downloading collection u'all-corpora'\n",
      "       | \n",
      "       | Downloading package abc to /root/nltk_data...\n",
      "       |   Unzipping corpora/abc.zip.\n",
      "       | Downloading package alpino to /root/nltk_data...\n",
      "       |   Unzipping corpora/alpino.zip.\n",
      "       | Downloading package biocreative_ppi to /root/nltk_data...\n",
      "       |   Unzipping corpora/biocreative_ppi.zip.\n",
      "       | Downloading package brown to /root/nltk_data...\n",
      "       |   Unzipping corpora/brown.zip.\n",
      "       | Downloading package brown_tei to /root/nltk_data...\n",
      "       |   Unzipping corpora/brown_tei.zip.\n",
      "       | Downloading package cess_cat to /root/nltk_data...\n",
      "       |   Unzipping corpora/cess_cat.zip.\n",
      "       | Downloading package cess_esp to /root/nltk_data...\n",
      "       |   Unzipping corpora/cess_esp.zip.\n",
      "       | Downloading package chat80 to /root/nltk_data...\n",
      "       |   Unzipping corpora/chat80.zip.\n",
      "       | Downloading package city_database to /root/nltk_data...\n",
      "       |   Unzipping corpora/city_database.zip.\n",
      "       | Downloading package cmudict to /root/nltk_data...\n",
      "       |   Unzipping corpora/cmudict.zip.\n",
      "       | Downloading package comtrans to /root/nltk_data...\n",
      "       | Downloading package conll2000 to /root/nltk_data...\n",
      "       |   Unzipping corpora/conll2000.zip.\n",
      "       | Downloading package conll2002 to /root/nltk_data...\n",
      "       |   Unzipping corpora/conll2002.zip.\n",
      "       | Downloading package conll2007 to /root/nltk_data...\n",
      "       | Downloading package crubadan to /root/nltk_data...\n",
      "       |   Unzipping corpora/crubadan.zip.\n",
      "       | Downloading package dependency_treebank to /root/nltk_data...\n",
      "       |   Unzipping corpora/dependency_treebank.zip.\n",
      "       | Downloading package floresta to /root/nltk_data...\n",
      "       |   Unzipping corpora/floresta.zip.\n",
      "       | Downloading package framenet_v15 to /root/nltk_data...\n",
      "       |   Unzipping corpora/framenet_v15.zip.\n",
      "       | Downloading package gazetteers to /root/nltk_data...\n",
      "       |   Unzipping corpora/gazetteers.zip.\n",
      "       | Downloading package genesis to /root/nltk_data...\n",
      "       |   Unzipping corpora/genesis.zip.\n",
      "       | Downloading package gutenberg to /root/nltk_data...\n",
      "       |   Unzipping corpora/gutenberg.zip.\n",
      "       | Downloading package ieer to /root/nltk_data...\n",
      "       |   Unzipping corpora/ieer.zip.\n",
      "       | Downloading package inaugural to /root/nltk_data...\n",
      "       |   Unzipping corpora/inaugural.zip.\n",
      "       | Downloading package indian to /root/nltk_data...\n",
      "       |   Unzipping corpora/indian.zip.\n",
      "       | Downloading package jeita to /root/nltk_data...\n",
      "       | Downloading package kimmo to /root/nltk_data...\n",
      "       |   Unzipping corpora/kimmo.zip.\n",
      "       | Downloading package knbc to /root/nltk_data...\n",
      "       | Downloading package lin_thesaurus to /root/nltk_data...\n",
      "       |   Unzipping corpora/lin_thesaurus.zip."
     ]
    }
   ],
   "source": [
    "#!pip install nltk\n",
    "#!pip install pandas\n",
    "import os, sys, re, json, time, shutil\n",
    "import itertools\n",
    "import collections\n",
    "from IPython.display import display\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "nltk.download()\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Pandas because pandas are awesome, and for pretty-printing\n",
    "import pandas as pd\n",
    "# Set pandas floating point display\n",
    "pd.set_option('float_format', lambda f: \"{0:.04f}\".format(f))\n",
    "\n",
    "# Helper libraries for this notebook\n",
    "import utils; reload(utils)\n",
    "import vocabulary; reload(vocabulary)\n",
    "import rnnlm; reload(rnnlm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNLM Model\n",
    "\n",
    "![RNNLM](RNNLM.png)\n",
    "\n",
    "Here's the basic spec for our model. We'll use the following notation:\n",
    "\n",
    "- $w^{(i)}$ for the $i^{th}$ word of the sequence (as an integer index)\n",
    "- $x^{(i)}$ for the vector representation of $w^{(i)}$\n",
    "- $h^{(i)}$ for the $i^{th}$ hidden state, with indices as in Section 4.8 of the async.\n",
    "- $o^{(i)}$ for the $i^{th}$ output state, which may or may not be the same as the hidden state\n",
    "\n",
    "Let $ h^{(-1)} = h^{init} $ be an initial state. For an input sequence of $n$ words and $i = 0, ..., n-1$, we have:\n",
    "\n",
    "- **Embedding layer:** $ x^{(i)} = W_{in}[w^{(i)}] $  \n",
    "- **Recurrent layer:** $ (h^{(i)}, o^{(i)}) = \\text{CellFunc}(x^{(i)}, h^{(i-1)}) $\n",
    "- **Output layer:** $ \\hat{P}(w^{(i+1)}) = \\text{softmax}(o^{(i)}W_{out} + b_{out}) $\n",
    " \n",
    "$\\text{CellFunc}$ can be an arbitrary function representing our recurrent cell - it can be a simple RNN cell, or something more complicated like an LSTM.\n",
    "\n",
    "We'll use these as shorthand for important dimensions:\n",
    "- `V` : vocabulary size\n",
    "- `H` : hidden state size = embedding size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) RNNLM Inputs and Parameters (9 points)\n",
    "\n",
    "**(written - no code)** Write your answers in the markdown cell below.  \n",
    "\n",
    "1. Let $\\text{CellFunc}$ be a simple RNN cell (see Section 4.8). Write the functional form in terms of nonlinearities and matrix multiplication. How many parameters (matrix or vector elements) are there for this cell, in terms of `V` and `H`?\n",
    "\n",
    "2. How many parameters are in the embedding layer? In the output layer?\n",
    "\n",
    "3. How many floating point operations are required to compute $\\hat{P}(w^{(i+1)})$ for a given target word $w^{(i+1)}$, assuming $h^{(i-1)}$ is already computed? How about for all target words?\n",
    "\n",
    "4. How does your answer to 3. change if we approximate $\\hat{P}(w^{(i+1)})$ with a sampled softmax with $k$ samples? How about if we use a hierarchical softmax?\n",
    "\n",
    "5. If you have an LSTM with $H = 200$ and use sampled softmax with $k = 100$, what part of the network takes up the most computation time during training?\n",
    "\n",
    "Note: for $A \\in \\mathbb{R}^{m \\times n}$ and $B \\in \\mathbb{R}^{n \\times l}$, computing the matrix product $AB$ takes $O(mnl)$ time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers for Part (a)\n",
    "You can use LaTeX to typeset math, e.g. `$ f(x) = x^2 $` will render as $ f(x) = x^2 $.\n",
    "\n",
    "1. theta=all, shape? embedding...embedding is part of the training....\n",
    "2. *Your text here!*\n",
    "3. *Your text here!*\n",
    "4. *Your text here!*\n",
    "5. *Your text here!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching and Truncated BPTT\n",
    "\n",
    "In theory, we model our RNN as operating on a single sequence of arbitrary length. During training, we'd run the RNN over that entire sequence, then backpropagate the errors from all the training labels. \n",
    "\n",
    "In practice, however, it's more common to operate on batches, and to perform *truncated backpropagation* on a fixed-length slice.\n",
    "\n",
    "Batching for an RNN means we'll run several copies of the RNN simultaneously, each with their own hidden state and outputs. Most TensorFlow functions are batch-aware, and expect the batch size as the first dimension.\n",
    "\n",
    "*Truncated backpropagation* means that we'll chop our sequences into blocks of fixed length - say, 20 - and run the RNN for only a fixed number `max_time` steps before we backpropagate errors. We'll still keep the final hidden state so that we can keep computing over the sequence, but we won't backpropagate across this boundary. For example, if our input was the sequence `1 2 3 4 5 6 7 8 9 10 11`, and `max_time=5`, we'd do:\n",
    "\n",
    "- Initialize `h0`\n",
    "- Run on block `[1 2 3 4 5]`\n",
    "- Backprop errors from targets `[2 3 4 5 6]`\n",
    "- Set `h0 = h_final`\n",
    "- Run on block `[6 7 8 9 10]`\n",
    "- Backprop errors from targets `[7 8 9 10 11]`\n",
    "\n",
    "And so on for longer sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Implementing the RNNLM (21 points)\n",
    "\n",
    "In order to better manage the model parameters, we'll implement our RNNLM in the `RNNLM` class in `rnnlm.py`. We've given you a skeleton of starter code for this, but the bulk of the implementation is left to you.\n",
    "\n",
    "Particularly, you'll need to implement three functions:\n",
    "- `BuildCoreGraph()` : the main RNN itself\n",
    "- `BuildTrainGraph()` : the training operations, including `train_loss_`, and `train_step_`\n",
    "- `BuildSamplerGraph()` : operations to generate output samples (`pred_samples_`)\n",
    "\n",
    "See `rnnlm.py` for more documentation.\n",
    "\n",
    "### Notes and Tips\n",
    "**`BuildCoreGraph`**  \n",
    "We recommend implementing the `FancyRNNCell` function as a wrapper to construct LSTM cells with (optional) dropout and multi-layer cells.\n",
    "\n",
    "You can experiment with different initializations, but random uniform or Xavier initialization (as in Part 0) should work well.\n",
    "\n",
    "**`BuildTrainGraph`**  \n",
    "You can use the softmax loss from `BuildCoreGraph` here, but we strongly recommend implementing an approximate (e.g. sampled) loss function for `train_loss_`. This will greatly speed up training.\n",
    "\n",
    "For training steps, you can use any optimizer, but we recommend  `tf.train.GradientDescentOptimizer` with gradient clipping (`tf.clip_by_global_norm`) or `tf.train.AdagradOptimizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may find the following API functions useful:\n",
    "- [tf.nn.rnn_cell](https://www.tensorflow.org/versions/r0.9/api_docs/python/rnn_cell.html#neural-network-rnn-cells)\n",
    "- [tf.nn.dynamic_rnn](https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#dynamic_rnn)\n",
    "- [tf.nn.sparse_softmax_cross_entropy_with_logits](https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#sparse_softmax_cross_entropy_with_logits)\n",
    "- [tf.nn.sampled_softmax_loss](https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#sampled_softmax_loss)\n",
    "- [tf.multinomial](https://www.tensorflow.org/versions/r0.11/api_docs/python/constant_op.html#multinomial)\n",
    "\n",
    "Additionally, you can expect to make heavy use of [tf.shape](https://www.tensorflow.org/versions/r0.11/api_docs/python/array_ops.html#shape) and [tf.reshape](https://www.tensorflow.org/versions/r0.11/api_docs/python/array_ops.html#reshape). Note especially that you can use `-1` as a dimension in `tf.reshape` to automatically infer the size. For example:\n",
    "```\n",
    "x = tf.zeros([5,10], dtype=tf.float32)\n",
    "x.reshape([-1,])  # shape [50,]\n",
    "x.reshape([1, -1])  # shape [1, 50]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import rnnlm; reload(rnnlm)\n",
    "\n",
    "# Clear old log directory\n",
    "shutil.rmtree(\"tf_summaries\", ignore_errors=True)\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "  tf.set_random_seed(42)\n",
    "\n",
    "  lm = rnnlm.RNNLM(V=10000, H=200, num_layers=2)\n",
    "  lm.BuildCoreGraph()\n",
    "  lm.BuildTrainGraph()\n",
    "  lm.BuildSamplerGraph()\n",
    "\n",
    "  summary_writer = tf.train.SummaryWriter(\"tf_summaries\", \n",
    "                                          tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Found more than one graph event per run. Overwriting the graph with the newest event.\n",
      "Starting TensorBoard 29 on port 6006\n",
      "(You can navigate to http://172.17.0.2:6006)\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:02] \"GET / HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:03] \"GET /webcomponentsjs/webcomponents-lite.min.js HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:03] \"GET /lib/css/global.css HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:03] code 404, message Not Found\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:03] \"GET /dist/bazel-html-imports.html HTTP/1.1\" 404 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:03] \"GET /dist/tf-tensorboard.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:03] \"GET /iron-icons/iron-icons.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:03] \"GET /paper-dialog/paper-dialog.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:03] \"GET /paper-checkbox/paper-checkbox.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:03] \"GET /paper-toolbar/paper-toolbar.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:04] \"GET /polymer/polymer.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:04] \"GET /paper-tabs/paper-tabs.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:04] \"GET /neon-animation/neon-animation-runner-behavior.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:04] \"GET /paper-dialog-behavior/paper-dialog-behavior.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:04] \"GET /paper-header-panel/paper-header-panel.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:04] \"GET /paper-button/paper-button.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:04] \"GET /paper-styles/default-theme.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:04] \"GET /paper-dialog-behavior/paper-dialog-shared-styles.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:04] \"GET /paper-styles/typography.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:04] \"GET /paper-behaviors/paper-checked-element-behavior.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:04] \"GET /iron-iconset-svg/iron-iconset-svg.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:04] \"GET /polymer/polymer-mini.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:04] \"GET /iron-icon/iron-icon.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:04] \"GET /iron-menu-behavior/iron-menubar-behavior.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:04] \"GET /iron-flex-layout/iron-flex-layout.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:04] \"GET /paper-icon-button/paper-icon-button.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:04] \"GET /paper-styles/color.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:05] \"GET /paper-tabs/paper-tabs-icons.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:05] \"GET /iron-resizable-behavior/iron-resizable-behavior.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:05] \"GET /iron-meta/iron-meta.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:05] \"GET /paper-tabs/paper-tab.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:05] \"GET /iron-overlay-behavior/iron-overlay-behavior.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:05] \"GET /neon-animation/neon-animatable-behavior.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:05] \"GET /paper-styles/shadow.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:05] \"GET /iron-behaviors/iron-control-state.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:05] \"GET /iron-behaviors/iron-button-state.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:05] \"GET /paper-behaviors/paper-ripple-behavior.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:05] \"GET /iron-checked-element-behavior/iron-checked-element-behavior.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:05] \"GET /iron-overlay-behavior/iron-overlay-manager.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:05] \"GET /iron-fit-behavior/iron-fit-behavior.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:05] \"GET /iron-a11y-keys-behavior/iron-a11y-keys-behavior.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:05] \"GET /neon-animation/animations/opaque-animation.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:05] \"GET /font-roboto/roboto.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:05] \"GET /iron-validatable-behavior/iron-validatable-behavior.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:05] \"GET /iron-overlay-behavior/iron-overlay-backdrop.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:05] \"GET /paper-behaviors/paper-inky-focus-behavior.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:05] \"GET /paper-material/paper-material.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:05] \"GET /neon-animation/neon-animation-behavior.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:05] \"GET /neon-animation/web-animations.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:05] \"GET /iron-form-element-behavior/iron-form-element-behavior.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:05] \"GET /paper-material/paper-material-shared-styles.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:05] \"GET /paper-behaviors/paper-button-behavior.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:05] \"GET /paper-ripple/paper-ripple.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:06] \"GET /iron-menu-behavior/iron-menu-behavior.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:06] \"GET /web-animations-js/web-animations-next-lite.min.js HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:06] \"GET /polymer/polymer-micro.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:06] \"GET /iron-selector/iron-multi-selectable.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:06] \"GET /iron-selector/iron-selectable.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:06] \"GET /iron-selector/iron-selection.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:06] \"GET /lodash/lodash.min.js HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:06] \"GET /paper-input/paper-input.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:06] \"GET /paper-slider/paper-slider.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:07] \"GET /paper-progress/paper-progress.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:07] \"GET /iron-range-behavior/iron-range-behavior.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:07] \"GET /paper-input/paper-input-behavior.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:07] \"GET /paper-input/paper-input-container.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:07] \"GET /iron-input/iron-input.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:07] \"GET /paper-input/paper-input-char-counter.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:07] \"GET /paper-input/paper-input-addon-behavior.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:07] \"GET /paper-input/paper-input-error.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:07] \"GET /iron-a11y-announcer/iron-a11y-announcer.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:07] \"GET /d3/d3.js HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:08] \"GET /paper-styles/paper-styles.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:08] \"GET /iron-flex-layout/classes/iron-flex-layout.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:08] \"GET /iron-flex-layout/classes/iron-shadow-flex-layout.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:08] \"GET /paper-dropdown-menu/paper-dropdown-menu.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:08] \"GET /paper-item/paper-item.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:08] \"GET /paper-menu/paper-menu.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:08] \"GET /paper-menu/paper-menu-shared-styles.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:08] \"GET /paper-dropdown-menu/paper-dropdown-menu-shared-styles.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:08] \"GET /paper-dropdown-menu/paper-dropdown-menu-icons.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:08] \"GET /paper-item/paper-item-behavior.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:08] \"GET /paper-item/paper-item-shared-styles.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:08] \"GET /paper-menu-button/paper-menu-button.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:08] \"GET /neon-animation/animations/fade-out-animation.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:08] \"GET /paper-menu-button/paper-menu-button-animations.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:08] \"GET /iron-dropdown/iron-dropdown.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:08] \"GET /neon-animation/animations/fade-in-animation.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:09] \"GET /iron-dropdown/iron-dropdown-scroll-manager.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:09] \"GET /iron-collapse/iron-collapse.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:09] \"GET /plottable/plottable.js HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:09] \"GET /plottable/plottable.css HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:09] \"GET /paper-toggle-button/paper-toggle-button.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:10] \"GET /graphlib/dist/graphlib.core.js HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:10] \"GET /dagre/dist/dagre.core.js HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:10] \"GET /iron-list/iron-list.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:10] \"GET /paper-item/all-imports.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:10] \"GET /paper-item/paper-item-body.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:10] \"GET /paper-item/paper-icon-item.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:11] \"GET /paper-tooltip/paper-tooltip.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:11] \"GET /paper-radio-group/paper-radio-group.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:11] \"GET /paper-radio-button/paper-radio-button.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:11] \"GET /data/runs HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:11] \"GET /data/runs HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:15] \"GET /data/runs HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:15] \"GET /data/runs HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [28/Oct/2016 12:05:15] \"GET /data/graph?run=.&limit_attr_size=1024&large_attrs_key=_too_large_attrs HTTP/1.1\" 200 -\n",
      "^CTraceback (most recent call last):\n",
      "  File \"/usr/local/bin/tensorboard\", line 11, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard/tensorboard.py\", line 151, in main\n",
      "    tb_server.serve_forever()\n",
      "  File \"/usr/lib/python2.7/SocketServer.py\", line 236, in serve_forever\n",
      "    poll_interval)\n",
      "  File \"/usr/lib/python2.7/SocketServer.py\", line 155, in _eintr_retry\n",
      "    return func(*args)\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir tf_summaries --port 6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above will load your implementation, construct the graph, and write a logdir for TensorBoard. You can bring up TensorBoard with:\n",
    "```\n",
    "tensorboard --logdir tf_summaries --port 6006\n",
    "```\n",
    "As usual, check http://localhost:6006/#graphs to inspect your implementation. Remember, judicious use of `tf.name_scope()` and/or `tf.variable_scope()` will greatly improve the visualization, and make code easier to debug."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Training your RNNLM (5 points)\n",
    "\n",
    "We'll give you data loader functions in **`utils.py`**. They work similarly to the loaders in the Week 4 notebook.\n",
    "\n",
    "Particularly, `utils.batch_generator` will return an iterator that yields minibatches in the correct format. Batches will be of size `[batch_size, max_time]`, and consecutive batches will line up along rows so that the final state of one batch can be used as the initial state for the next.\n",
    "\n",
    "For example, using a toy corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input words:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w_0</th>\n",
       "      <th>w_1</th>\n",
       "      <th>w_2</th>\n",
       "      <th>w_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>Mary</td>\n",
       "      <td>had</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>The</td>\n",
       "      <td>lamb</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   w_0   w_1   w_2  w_3\n",
       "0  <s>  Mary   had    a\n",
       "1  <s>   The  lamb  was"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w_0</th>\n",
       "      <th>w_1</th>\n",
       "      <th>w_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>little</td>\n",
       "      <td>lamb</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>white</td>\n",
       "      <td>as</td>\n",
       "      <td>snow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      w_0   w_1   w_2\n",
       "0  little  lamb     .\n",
       "1   white    as  snow"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target words:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_0</th>\n",
       "      <th>y_1</th>\n",
       "      <th>y_2</th>\n",
       "      <th>y_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mary</td>\n",
       "      <td>had</td>\n",
       "      <td>a</td>\n",
       "      <td>little</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The</td>\n",
       "      <td>lamb</td>\n",
       "      <td>was</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    y_0   y_1  y_2     y_3\n",
       "0  Mary   had    a  little\n",
       "1   The  lamb  was   white"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_0</th>\n",
       "      <th>y_1</th>\n",
       "      <th>y_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lamb</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>as</td>\n",
       "      <td>snow</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    y_0   y_1  y_2\n",
       "0  lamb     .  <s>\n",
       "1    as  snow    ."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "toy_corpus = \"<s> Mary had a little lamb . <s> The lamb was white as snow . <s>\"\n",
    "toy_corpus = np.array(toy_corpus.split())\n",
    "\n",
    "print \"Input words:\"\n",
    "bi = utils.batch_generator(toy_corpus, batch_size=2, max_time=4)\n",
    "for i, (w,y) in enumerate(bi):\n",
    "  utils.pretty_print_matrix(w, cols=[\"w_%d\" % d for d in range(w.shape[1])], dtype=object)\n",
    "\n",
    "print \"Target words:\"\n",
    "bi = utils.batch_generator(toy_corpus, batch_size=2, max_time=4)\n",
    "for i, (w,y) in enumerate(bi):\n",
    "  utils.pretty_print_matrix(y, cols=[\"y_%d\" % d for d in range(w.shape[1])], dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the data we feed to our model will be word indices, but the shape will be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implement the `run_epoch` function\n",
    "We've given you some starter code for logging progress; fill this in with actual call(s) to `session.run` with the appropriate arguments to run a training step. \n",
    "\n",
    "Be sure to handle the initial state properly at the beginning of an epoch, and remember to carry forward the final state into the next batch.\n",
    "\n",
    "**Note:** we provide a `train=True` flag to enable train mode. If `train=False`, this function can also be used for scoring the dataset - see `score_dataset()` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epoch(lm, session, batch_iterator, train=False,\n",
    "              verbose=False, tick_s=10, \n",
    "              keep_prob=1.0, learning_rate=0.1):\n",
    "  start_time = time.time()\n",
    "  tick_time = start_time  # for showing status\n",
    "  total_cost = 0.0  # total cost, summed over all words\n",
    "  total_words = 0\n",
    "\n",
    "  if train:\n",
    "    train_op = lm.train_step_\n",
    "    keep_prob = keep_prob\n",
    "    loss = lm.train_loss_\n",
    "  else:\n",
    "    train_op = tf.no_op()\n",
    "    keep_prob = 1.0  # no dropout at test time\n",
    "    loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "\n",
    "  for i, (w, y) in enumerate(batch_iterator):\n",
    "    cost = 0.0\n",
    "    #### YOUR CODE HERE ####\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    #### END(YOUR CODE) ####\n",
    "    total_cost += cost\n",
    "    total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "    ##\n",
    "    # Print average loss-so-far for epoch\n",
    "    # If using train_loss_, this may be an underestimate.\n",
    "    if verbose and (time.time() - tick_time >= tick_s):\n",
    "      avg_cost = total_cost / total_words\n",
    "      avg_wps = total_words / (time.time() - start_time)\n",
    "      print \"[batch %d]: seen %d words at %d wps, loss = %.3f\" % (i,\n",
    "          total_words, avg_wps, avg_cost)\n",
    "      tick_time = time.time()  # reset time ticker\n",
    "\n",
    "  return total_cost / total_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Run Training\n",
    "\n",
    "We'll give you the outline of the training procedure, but you'll need to fill in a call to your `run_epoch` function. \n",
    "\n",
    "At the end of training, we use a `tf.train.Saver` to save a copy of the model to `./tf_saved/rnnlm_trained`. You'll want to load this from disk to work on later parts of the assignment; see **part (d)** for an example of how this is done.\n",
    "\n",
    "#### Tuning Hyperparameters\n",
    "With a sampled softmax loss, the default hyperparameters should train 5 epochs in under 10 minutes on a 16-core Google Cloud Compute instance, and reach a training set perplexity below 200.\n",
    "\n",
    "However, it's possible to do significantly better. Try experimenting with multiple RNN layers (`num_layers` > 1) or a larger hidden state - though you may also need to adjust the learning rate and number of epochs for a larger model.\n",
    "\n",
    "You can also experiment with a larger vocabulary. This will look worse for perplexity, but will be a better model overall as it won't treat so many words as `<unk>`.\n",
    "\n",
    "#### Submitting your model\n",
    "You should submit your trained model along with the assignment. Do:\n",
    "```\n",
    "git add tf_saved/rnnlm_trained tf_saved/rnnlm_trained.meta\n",
    "git commit -m \"Adding trained model.\"\n",
    "```\n",
    "Unless you train a very large model, these files should be < 50 MB and no problem for git to handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource u'corpora/brown' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-3ed92f409eb2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load the dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mV\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_corpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"brown\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mV\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mV\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/notebooks/assignment1/part2/utils.pyc\u001b[0m in \u001b[0;36mload_corpus\u001b[1;34m(name, split, V, shuffle)\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[1;34m\"\"\"Load a named corpus and split train/test along sentences.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_corpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m     \u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mV\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m     \u001b[0mtrain_sentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_sentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_train_test_sents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[0mtrain_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess_sentences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_sentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/notebooks/assignment1/part2/utils.pyc\u001b[0m in \u001b[0;36mbuild_vocab\u001b[1;34m(corpus, V)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mV\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m     \u001b[0mtoken_feed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcanonicalize_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m     \u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_feed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mV\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/nltk/corpus/util.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m     97\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/nltk/corpus/util.pyc\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'corpora/%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource u'corpora/brown' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "V = 10000\n",
    "vocab, train_ids, test_ids = utils.load_corpus(\"brown\", split=0.8, V=V, shuffle=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "max_time = 20\n",
    "batch_size = 50\n",
    "learning_rate = 0.5\n",
    "keep_prob = 1.0\n",
    "num_epochs = 5\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(V=V, \n",
    "                    H=100, \n",
    "                    num_layers=1)\n",
    "\n",
    "trained_filename = 'tf_saved/rnnlm_trained'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def score_dataset(lm, session, ids, name=\"Data\"):\n",
    "  bi = utils.batch_generator(ids, batch_size=100, max_time=100)\n",
    "  cost = run_epoch(lm, session, bi, \n",
    "                   learning_rate=1.0, keep_prob=1.0, \n",
    "                   train=False, verbose=False, tick_s=3600)\n",
    "  print \"%s: avg. loss: %.03f  (perplexity: %.02f)\" % (name, cost, np.exp(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Will print status every this many seconds\n",
    "print_interval = 5\n",
    "\n",
    "# Clear old log directory\n",
    "shutil.rmtree(\"tf_summaries\", ignore_errors=True)\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session() as session:\n",
    "  # Seed RNG for repeatability\n",
    "  tf.set_random_seed(42)\n",
    "  \n",
    "  with tf.variable_scope(\"model\", reuse=None):\n",
    "    lm = rnnlm.RNNLM(**model_params)\n",
    "    lm.BuildCoreGraph()\n",
    "    lm.BuildTrainGraph()\n",
    "  \n",
    "  session.run(tf.initialize_all_variables())\n",
    "  saver = tf.train.Saver()\n",
    "  \n",
    "  for epoch in xrange(1,num_epochs+1):\n",
    "    t0_epoch = time.time()\n",
    "    bi = utils.batch_generator(train_ids, batch_size, max_time)\n",
    "    print \"[epoch %d] Starting epoch %d\" % (epoch, epoch)\n",
    "    #### YOUR CODE HERE ####\n",
    "    \n",
    "    # Run a training epoch.\n",
    "    \n",
    "    #### END(YOUR CODE) ####\n",
    "    print \"[epoch %d] Completed in %s\" % (epoch, utils.pretty_timedelta(since=t0_epoch))\n",
    "    \n",
    "    ##\n",
    "    # score_dataset will run a forward pass over the entire dataset\n",
    "    # and report perplexity scores. This can be slow (around 1/2 to \n",
    "    # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "    # to speed up training on a slow machine. Be sure to run it at the \n",
    "    # end to evaluate your score.\n",
    "    print (\"[epoch %d]\" % epoch),\n",
    "    score_dataset(lm, session, train_ids, name=\"Train set\")\n",
    "    print (\"[epoch %d]\" % epoch),\n",
    "    score_dataset(lm, session, test_ids, name=\"Test set\")\n",
    "    print \"\"\n",
    "    \n",
    "    # Save a checkpoint\n",
    "    saver.save(session, 'tf_saved/rnnlm', global_step=epoch)\n",
    "    \n",
    "  # Save final model\n",
    "  saver.save(session, trained_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) Sampling Sentences (5 points)\n",
    "\n",
    "If you didn't already in **part (b)**, implement the `BuildSamplerGraph()` method in `rnnlm.py` See the function docstring for more information.\n",
    "\n",
    "#### Implement the `sample_step()` method below (5 points)\n",
    "This should access the Tensors you create in `BuildSamplerGraph()`. Given an input batch and initial states, it should return a vector of shape `[batch_size,]` containing sampled indices for the next word of each batch sequence.\n",
    "\n",
    "Run the method using the provided code to generate 10 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_step(lm, session, input_w, initial_h):\n",
    "  \"\"\"Run a single RNN step and return sampled predictions.\n",
    "  \n",
    "  Args:\n",
    "    lm : rnnlm.RNNLM\n",
    "    session: tf.Session\n",
    "    input_w : [batch_size] list of indices\n",
    "    initial_h : [batch_size, hidden_dims]\n",
    "  \n",
    "  Returns:\n",
    "    final_h : final hidden state, compatible with initial_h\n",
    "    samples : [batch_size, 1] vector of indices\n",
    "  \"\"\"\n",
    "  #### YOUR CODE HERE ####\n",
    "  # Reshape input to column vector\n",
    "  input_w = np.array(input_w, dtype=np.int32).reshape([-1,1])\n",
    "  \n",
    "  # Run sample ops\n",
    "  \n",
    "  #### END(YOUR CODE) ####\n",
    "  return final_h, samples[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Same as above, but as a batch\n",
    "max_steps = 20\n",
    "num_samples = 10\n",
    "random_seed = 42\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session() as session:\n",
    "  # Seed RNG for repeatability\n",
    "  tf.set_random_seed(random_seed)\n",
    "\n",
    "  with tf.variable_scope(\"model\", reuse=None):\n",
    "    lm = rnnlm.RNNLM(**model_params)\n",
    "    lm.BuildCoreGraph()\n",
    "    lm.BuildSamplerGraph()\n",
    "\n",
    "  # Load the trained model\n",
    "  saver = tf.train.Saver()\n",
    "  saver.restore(session, trained_filename)\n",
    "\n",
    "  # Make initial state for a batch with batch_size = num_samples\n",
    "  w = np.repeat([[vocab.START_ID]], num_samples, axis=0)\n",
    "  h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "  # We'll take one step for each sequence on each iteration \n",
    "  for i in xrange(max_steps):\n",
    "    state, y = sample_step(lm, session, w[:,-1:], h)\n",
    "    w = np.hstack((w,y))\n",
    "\n",
    "  # Print generated sentences\n",
    "  for row in w:\n",
    "    for i, word_id in enumerate(row):\n",
    "      print vocab.id_to_word[word_id],\n",
    "        if (i != 0) and (word_id == vocab.START_ID):\n",
    "          break\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## (e) Linguistic Properties (5 points)\n",
    "\n",
    "Now that we've trained our RNNLM, let's test a few properties of the model to see how well it learns linguistic phenomena. We'll do this with a scoring task: given two or more test sentences, our model should score the more plausible (or more correct) sentence with a higher log-probability.\n",
    "\n",
    "We'll define a scoring function to help us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_seq(lm, session, seq, vocab):\n",
    "  \"\"\"Score a sequence of words. Returns total log-probability.\"\"\"\n",
    "  padded_ids = vocab.words_to_ids(utils.canonicalize_words([\"<s>\"] + seq, \n",
    "                                                           wordset=vocab.word_to_id))\n",
    "  w = np.reshape(padded_ids[:-1], [1,-1])\n",
    "  y = np.reshape(padded_ids[1:],  [1,-1])\n",
    "  h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "  feed_dict = {lm.input_w_:w,\n",
    "               lm.target_y_:y,\n",
    "               lm.initial_h_:h,\n",
    "               lm.dropout_keep_prob_: 1.0}\n",
    "  # Return log(P(seq)) = -1*loss\n",
    "  return -1*session.run(lm.loss_, feed_dict)\n",
    "\n",
    "def load_and_score(inputs, sort=False):\n",
    "  \"\"\"Load the trained model and score the given words.\"\"\"\n",
    "  with tf.Graph().as_default(), tf.Session() as session:  \n",
    "    with tf.variable_scope(\"model\", reuse=None):\n",
    "      lm = rnnlm.RNNLM(**model_params)\n",
    "      lm.BuildCoreGraph()\n",
    "        \n",
    "    # Load the trained model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(session, trained_filename)\n",
    "  \n",
    "    if isinstance(inputs[0], str) or isinstance(inputs[0], unicode):\n",
    "      inputs = [inputs]\n",
    "\n",
    "    # Actually run scoring\n",
    "    results = []\n",
    "    for words in inputs:\n",
    "      score = score_seq(lm, session, words, vocab)\n",
    "      results.append((score, words))\n",
    "    \n",
    "    # Sort if requested\n",
    "    if sort: results = sorted(results, reverse=True)\n",
    "    \n",
    "    # Print results\n",
    "    for score, words in results:\n",
    "      print \"\\\"%s\\\" : %.02f\" % (\" \".join(words), score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sents = [\"once upon a time\",\n",
    "         \"the quick brown fox jumps over the lazy dog\"]\n",
    "load_and_score([s.split() for s in sents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Number agreement\n",
    "\n",
    "Compare **\"the boy and the girl [are/is]\"**. Which is more plausible according to your model?\n",
    "\n",
    "If your model doesn't order them correctly (*this is OK*), why do you think that might be? (answer in cell below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "\n",
    "\n",
    "#### END(YOUR CODE) ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer to part 1. question(s)\n",
    "\n",
    "*Answer to above question(s).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Type/semantic agreement\n",
    "\n",
    "Compare:\n",
    "- **\"peanuts are my favorite kind of [nut/vegetable]\"**\n",
    "- **\"when I'm hungry I really prefer to [eat/drink]\"**\n",
    "\n",
    "Of each pair, which is more plausible according to your model?\n",
    "\n",
    "How would you expect a 3-gram language model to perform at this example? How about a 5-gram model? (answer in cell below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### END(YOUR CODE) ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer to part 2. question(s)\n",
    "\n",
    "*Answer to above question(s).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Adjective ordering (just for fun)\n",
    "\n",
    "Let's repeat the exercise from Week 2:\n",
    "\n",
    "![Adjective Order](adjective_order.jpg)\n",
    "*source: https://twitter.com/MattAndersonBBC/status/772002757222002688?lang=en*\n",
    "\n",
    "We'll consider a toy example (literally), and consider all possible adjective permutations.\n",
    "\n",
    "Note that this is somewhat sensitive to training, and even a good language model might not get it all correct. Why might the NN fail, if the trigram model from week2 was able to solve it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prefix = \"I have lots of\".split()\n",
    "noun = \"toys\"\n",
    "adjectives = [\"square\", \"green\", \"plastic\"]\n",
    "inputs = []\n",
    "for adjs in itertools.permutations(adjectives):\n",
    "  words = prefix + list(adjs) + [noun]\n",
    "  inputs.append(words)\n",
    "    \n",
    "load_and_score(inputs, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
