{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Update:** We've improved the model schematic, and added clarifications to the documentation in both this notebook and in `rnnlm.py`. Fetch the latest version from GitHub, or look at the diffs to see what's new.\n",
    "\n",
    "**Update:** We've provided the solution implementation of `MakeFancyRNNCell`; hopefully this will make things easier! For an illustration of how a multi-layer RNN cell works, see [this diagram](RNNLM - multicell.png). The \"cell\" from `MakeFancyRNNCell` with `num_layers = 2` is the unit inside the dashed green box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1, Part 2: RNN Language Model\n",
    "**(45 points total)**\n",
    "\n",
    "In this part of the assignment, you'll implement a recurrent neural network language model. This class of models represents the cutting edge in language modeling, and what you implement will be include many of the same features.\n",
    "\n",
    "As a reference, you may want to review the following papers:\n",
    "\n",
    "- [Recurrent neural network based language model](http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf) (Mikolov, et al. 2010)\n",
    "- [Exploring the Limits of Language Modeling](http://arxiv.org/pdf/1602.02410.pdf) (Jozefowicz, et al. 2016)\n",
    "\n",
    "You'll be writing a fair amount of TensorFlow code, so you may want to review the [Week 1 TensorFlow tutorial](../week1/TensorFlow%20Tutorial.ipynb) or the [Week 4 notebook](../week4/Neural%20Probabilistic%20Language%20Model.ipynb) for review. For documentation on specific functions, consult the [TensorFlow API reference](https://www.tensorflow.org/versions/r0.10/api_docs/python/index.html).\n",
    "\n",
    "Be sure you're using a recent installation of **TensorFlow version 0.9.0 or higher**.\n",
    "\n",
    "#### Note on Indentation\n",
    "\n",
    "This notebook (as well as rnnlm.py) uses 2-space indentation, instead of the 4 spaces that is Python's default. Why? It makes lines shorter, which is handy if you have a lot of nested scopes. Some people will yell at you for doing this, but the instructors don't care either way.\n",
    "\n",
    "You can follow [these instructions](http://jupyter-notebook.readthedocs.io/en/latest/frontend_config.html#example-changing-the-notebook-s-default-indentation) to configure Jupyter to be cool about this. In Chrome, you can open a JavaScript console by going to Menu -> More Tools -> Developer Tools and clicking the \"Console\" tab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 Overview\n",
    "\n",
    "- **(a)** RNNLM Inputs and Parameters (written questions)\n",
    "- **(b)** Implementing the RNNLM\n",
    "- **(c)** Training your RNNLM\n",
    "- **(d)** Sampling Sentences\n",
    "- **(e)** Linguistic Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'rnnlm' from 'rnnlm.pyc'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys, re, json, time, shutil\n",
    "import itertools\n",
    "import collections\n",
    "from IPython.display import display\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Pandas because pandas are awesome, and for pretty-printing\n",
    "import pandas as pd\n",
    "# Set pandas floating point display\n",
    "pd.set_option('float_format', lambda f: \"{0:.04f}\".format(f))\n",
    "\n",
    "# Helper libraries for this notebook\n",
    "import utils; reload(utils)\n",
    "import vocabulary; reload(vocabulary)\n",
    "import rnnlm; reload(rnnlm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNLM Model\n",
    "\n",
    "![RNNLM](RNNLM - layers.png)\n",
    "\n",
    "Here's the basic spec for our model. We'll use the following notation:\n",
    "\n",
    "- $w^{(i)}$ for the $i^{th}$ word of the sequence (as an integer index)\n",
    "- $x^{(i)}$ for the vector representation of $w^{(i)}$\n",
    "- $h^{(i)}$ for the $i^{th}$ hidden state, with indices as in Section 4.8 of the async.\n",
    "- $o^{(i)}$ for the $i^{th}$ output state, which may or may not be the same as the hidden state\n",
    "- $y^{(i)}$ for the $i^{th}$ target word, which for a language model is always equal to $w^{(i+1)}$\n",
    "\n",
    "Let $ h^{(-1)} = h^{init} $ be an initial state. For an input sequence of $n$ words and $i = 0, ..., n-1$, we have:\n",
    "\n",
    "- **Embedding layer:** $ x^{(i)} = W_{in}[w^{(i)}] $\n",
    "- **Recurrent layer:** $ (h^{(i)}, o^{(i)}) = \\text{CellFunc}(x^{(i)}, h^{(i-1)}) $\n",
    "- **Output layer:** $\\hat{P}(y^{(i)}) = \\hat{P}(w^{(i+1)}) = \\text{softmax}(o^{(i)}W_{out} + b_{out}) $\n",
    " \n",
    "$\\text{CellFunc}$ can be an arbitrary function representing our recurrent cell - it can be a simple RNN cell, or something more complicated like an LSTM, or even a stacked multi-layer cell.\n",
    "\n",
    "We'll use these as shorthand for important dimensions:\n",
    "- `V` : vocabulary size\n",
    "- `H` : hidden state size = embedding size\n",
    "\n",
    "It may be convenient to deal with the logits of the output layer, which are the un-normalized inputs to the softmax:\n",
    "$$ \\text{logits}^{(i)} = o^{(i)}W_{out} + b_{out} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) RNNLM Inputs and Parameters (9 points)\n",
    "\n",
    "**(written - no code)** Write your answers in the markdown cell below.\n",
    "\n",
    "You should use big-O notation when appropriate (i.e. computing $\\exp(\\mathbf{v})$ for a vector $\\mathbf{v}$ of length $n$ is $O(n)$ operations).\n",
    "\n",
    "1. Let $\\text{CellFunc}$ be a simple RNN cell (see Section 4.8). Write the functional form in terms of nonlinearities and matrix multiplication. How many parameters (matrix or vector elements) are there for this cell, in terms of `V` and `H`?\n",
    "\n",
    "2. How many parameters are in the embedding layer? In the output layer? (By parameters, we mean total number of matrix elements across all train-able tensors.)\n",
    "\n",
    "3. How many floating point operations are required to compute $\\hat{P}(w^{(i+1)})$ for a given target word $w^{(i+1)}$, assuming $w^{(i)}$ given and $h^{(i-1)}$ already computed? How about for all target words?\n",
    "\n",
    "4. How does your answer to 3. change if we approximate $\\hat{P}(w^{(i+1)})$ with a sampled softmax with $k$ samples? How about if we use a hierarchical softmax? (*Recall that hierarchical softmax makes a series of left/right decisions using a binary classifier $P_s(\\text{right}) = \\sigma(u_s \\cdot o^{(i)} + b_s) \\ge 0.5$ at each split $s$ in the tree.*)\n",
    "\n",
    "5. If you have an LSTM with $H = 200$ and use sampled softmax with $k = 100$, what part of the network takes up the most computation time during training? (*Choose \"embedding layer\", \"recurrent layer\", or \"output layer\"*)\n",
    "\n",
    "Note: for $A \\in \\mathbb{R}^{m \\times n}$ and $B \\in \\mathbb{R}^{n \\times l}$, computing the matrix product $AB$ takes $O(mnl)$ time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers for Part (a)\n",
    "You can use LaTeX to typeset math, e.g. `$ f(x) = x^2 $` will render as $ f(x) = x^2 $.\n",
    "\n",
    "1. $\\text{CellFunc}(x^{(i)}, h^{(i-1)}) = tanh(W_{in} x (x^{(i)}, h^{(i-1)}) + b^{(i)})$\n",
    "Focusing on the cell, the incoming vector's width is the state width + the embedding width, or $H + H = 2H$, which is also the first dimension of the alphine layer's $W$.  The second dimension for $W$ is just $H$, (the hidden layer size), so is the size of $b$.  Therefore, the total number of parameters within each cell is number of $W$ paramenters plus number of $b$ parameters, which is $2xH x H + H = \\mathbf{2H^2 + H}$\n",
    "2. The input size for the embedding layer is going to be $V$, the totaly number vocabularies, or the word vector size.  The output from the embedding layer has size $H$ in our case (the embedding size).  The bias size, $b_em$ for the embedding layer is the same as the output's, i.e. $V$.  Therefore, the total number of parameters for the embedding layer is the sum of the number of parameters in the weight matrix, which is $V x H$, and the number of parameters in $b_em$, which is $V$.  Therefore, the answer for the embedding layer is is $\\mathbf{VH+V}$.\n",
    "As to the output layer, the incoming nodes (from the output of the recurrent layer, i.e. the hidden nodes), has size of $H$, the outgoing nodes has size of $V$.  Therefore, there should be $\\mathbf{VH + V}$ parameters for the output layer, where the second $V$ is due to the output bias, (ref: $ \\hat{P}(w^{(i+1)}) = \\text{softmax}(o^{(i)}W_{out} + b_{out})$ and softmax does not incur new parameters).  \n",
    "3. Since $\\hat{P}(w^{(i+1)}) = \\text{softmax}(o^{(i)}W_{out} + b_{out}$, the number of floating point operations in $\\hat{P}(w^{(i+1)})$ for a given target word $w^{(i+1)}$, (assuming $h^{(i-1)}$ is already computed), should be tallied in three parts: embedding, recurrent, and output layers. For the *embedding layer*, we are going to assume the lookup matrix is already available, so the embedding layer part is just an index lookup from $w^{(i)}$ to $x^{()}$, thus no floating point. \n",
    "For the *recurrent layer*, we have $2HxH$ for the matrix, and $b_rc$ bias vector, *plus* the $tanh$, which is $2H^2 + (H-1)H$ for the matmul, $H$ for the $b_rc$, and $H$ for the $tanh$.  Together, that is $3H^2 - H + H + H =$ $\\mathbf{3H^2 + H}$ floating piont operations.\n",
    "For the *output layer*, given that $out=o^{(i)}W_{out} + b_{out}$, plus the softmax itself, defined as $\\frac{exp(out_i)}{\\sum_{j=1}^{V}exp(out_j)}$, we get $VH$ multiplications plus $(H-1)V$ additions for the matmul, plus $V$ for the  $b_out$, .  Though this is just about $h^{(i)}$, the softmax need to calculate the whole denominator, i.e., $exp()$ then sigma (addition), which is $V$ $exp()$ plus $V-1$ addition floating point operations.  Then the single division (the numerator should already be done earlier in the denominator).  So, for softmax, it is $V + V - 1 + 1 = 2V$  Together, the total for the output layer is $\\mathbf{2VH + 2V}$ floating points.  To add all three layers up, we have $0 + 3H^2 + H + 2VH + 2V = \\mathbf{3H^2 + 2VH + H + 2V}$ total floating point operations.  For the whole $V$ target words, you need to add $V - 1$ more divisions to make the total $\\mathbf{3H^2 + 2VH + H + 3V -1}$.\n",
    "4. First, with the last step in softmax replaced with the sampling method, we are virtually saying, instead of $V$, we are only going to involve $k$ for the logit and softmax.  Thus, the counts for single and whole target becomes $\\mathbf{3H^2 + 2kH + H + 2k}$, and $\\mathbf{3H^2 + 2kH + H + 3k -1}$ respectively.  For the heirarchic softmax situation, the $V$ is now $log_2V$.  Or, the tallies for single and whole are $\\mathbf{3H^2 + 2log_2(V)H + H + 2log_2(V)}$, and $\\mathbf{3H^2 + 2log_2(V)H + H + 3log_2(V) -1}$ respectively.\n",
    "5. With the sampling softmax and assuming the embedding layer involves just a loop up from $w^{(i)}$ as the integer index, and training potentially involves $k$ vocabularies, the difference between *recurrent layer* and *output layer* is $\\mathbf{3H^2 + H}$ vs $\\mathbf{2kH + 2k}$ from a.2 and a.4.  Plug $H = 200$ and $k = 100$, we see that it's around $3 x 200 x 200 + 200 = 120200$ vs $2 x 100 x 200 + 2 x 100 = 40200$.  So, the _*recurrent layer*_ bears the most computation in training among the three."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching and Truncated BPTT\n",
    "\n",
    "In theory, we model our RNN as operating on a single sequence of arbitrary length. During training, we'd run the RNN over that entire sequence, then backpropagate the errors from all the training labels. \n",
    "\n",
    "In practice, however, it's more common to operate on batches, and to perform *truncated backpropagation* on a fixed-length slice. This means our inputs $w$ and targets $y$ will be 2D arrays of shape `[batch_size, max_time]`:\n",
    "\n",
    "![RNNLM - batching](RNNLM - batching.png)\n",
    "\n",
    "Batching for an RNN means we'll run several copies of the RNN simultaneously, each with their own hidden state and outputs. Most TensorFlow functions are batch-aware, and expect the batch size as the first dimension.\n",
    "\n",
    "*Truncated backpropagation* through time means that we'll chop our sequences into blocks of fixed length - say, 20 - and run the RNN for only a fixed number `max_time` steps before we backpropagate errors. We'll still keep the final hidden state so that we can keep computing over the sequence, but we won't backpropagate across this boundary. For example, if our input was the sequence `1 2 3 4 5 6 7 8 9 10 11`, and `max_time=5`, we'd do:\n",
    "\n",
    "- Initialize `h0`\n",
    "- Run on block `[1 2 3 4 5]`\n",
    "- Backprop errors from targets `[2 3 4 5 6]`\n",
    "- Set `h0 = h_final`\n",
    "- Run on block `[6 7 8 9 10]`\n",
    "- Backprop errors from targets `[7 8 9 10 11]`\n",
    "\n",
    "And so on for longer sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Implementing the RNNLM (21 points)\n",
    "\n",
    "In order to better manage the model parameters, we'll implement our RNNLM in the `RNNLM` class in `rnnlm.py`. We've given you a skeleton of starter code for this, but the bulk of the implementation is left to you.\n",
    "\n",
    "Particularly, you'll need to implement three functions:\n",
    "- `BuildCoreGraph()` : the main RNN itself\n",
    "- `BuildTrainGraph()` : the training operations, including `train_loss_`, and `train_step_`\n",
    "- `BuildSamplerGraph()` : operations to generate output samples (`pred_samples_`)\n",
    "\n",
    "See `rnnlm.py` for more documentation.\n",
    "\n",
    "### Notes and Tips\n",
    "**`BuildCoreGraph`**\n",
    "We recommend implementing the `MakeFancyRNNCell` function as a wrapper to construct LSTM cells with (optional) dropout and multi-layer cells.\n",
    "\n",
    "You should use **`tf.nn.dynamic_rnn`** to build your recurrent layer. It takes care of making the recurrent connections and ensuring that the computation is done in the right (temporal) order, and gives you a nice wrapper that can take inputs of shape `[batch_size, max_time, ...]`.\n",
    "\n",
    "You'll need to provide initializations for your variables in the embedding layer and the output layer; we recommend random uniform or Xavier initialization (as in Part 0). The `tf.nn.rnn_cell` functions will automatically handle initialization of the internal cell variables (i.e. the LSTM matricies).\n",
    "\n",
    "**`BuildTrainGraph`**  \n",
    "You can use the softmax loss from `BuildCoreGraph` here, but we strongly recommend implementing an approximate (e.g. sampled) loss function for `train_loss_`. This will greatly speed up training.\n",
    "\n",
    "For training steps, you can use any optimizer, but we recommend  `tf.train.GradientDescentOptimizer` with gradient clipping (`tf.clip_by_global_norm`) or `tf.train.AdagradOptimizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may find the following API functions useful:\n",
    "- [tf.nn.rnn_cell](https://www.tensorflow.org/versions/r0.11/api_docs/python/rnn_cell.html#RNNCell) (particularly `cell.zero_state`)\n",
    "- [tf.nn.dynamic_rnn](https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#dynamic_rnn)\n",
    "- [tf.nn.sparse_softmax_cross_entropy_with_logits](https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#sparse_softmax_cross_entropy_with_logits)\n",
    "- [tf.nn.sampled_softmax_loss](https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#sampled_softmax_loss)\n",
    "- [tf.multinomial](https://www.tensorflow.org/versions/r0.11/api_docs/python/constant_op.html#multinomial)\n",
    "\n",
    "Additionally, you can expect to make heavy use of [tf.shape](https://www.tensorflow.org/versions/r0.11/api_docs/python/array_ops.html#shape) and [tf.reshape](https://www.tensorflow.org/versions/r0.11/api_docs/python/array_ops.html#reshape). Note especially that you can use `-1` as a dimension in `tf.reshape` to automatically infer the size. For example:\n",
    "```\n",
    "x = tf.zeros([5,10], dtype=tf.float32)\n",
    "x.reshape([-1,])  # shape [50,]\n",
    "x.reshape([1, -1])  # shape [1, 50]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import rnnlm; reload(rnnlm)\n",
    "\n",
    "# Clear old log directory\n",
    "shutil.rmtree(\"tf_summaries\", ignore_errors=True)\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "  tf.set_random_seed(42)\n",
    "\n",
    "  lm = rnnlm.RNNLM(V=10000, H=200, num_layers=2)\n",
    "  lm.BuildCoreGraph() \n",
    "\n",
    "  lm.em_lu_.get_shape()\n",
    "  lm.BuildTrainGraph()\n",
    "  lm.BuildSamplerGraph()\n",
    "\n",
    "  summary_writer = tf.train.SummaryWriter(\"tf_summaries\", \n",
    "                                          tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting TensorBoard 29 on port 6006\n",
      "(You can navigate to http://172.17.0.2:6006)\n",
      "WARNING:tensorflow:Found more than one graph event per run. Overwriting the graph with the newest event.\n",
      "WARNING:tensorflow:Found more than one graph event per run. Overwriting the graph with the newest event.\n",
      "WARNING:tensorflow:Found more than one graph event per run. Overwriting the graph with the newest event.\n",
      "WARNING:tensorflow:Found more than one graph event per run. Overwriting the graph with the newest event.\n",
      "WARNING:tensorflow:Found more than one graph event per run. Overwriting the graph with the newest event.\n",
      "WARNING:tensorflow:Found more than one graph event per run. Overwriting the graph with the newest event.\n",
      "WARNING:tensorflow:Found more than one graph event per run. Overwriting the graph with the newest event.\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:33] \"GET / HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:33] \"GET /webcomponentsjs/webcomponents-lite.min.js HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:33] \"GET /lib/css/global.css HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:33] code 404, message Not Found\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:33] \"GET /dist/bazel-html-imports.html HTTP/1.1\" 404 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:33] \"GET /dist/tf-tensorboard.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:33] \"GET /polymer/polymer.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:33] \"GET /paper-dialog/paper-dialog.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:33] \"GET /paper-tabs/paper-tabs.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:33] \"GET /paper-toolbar/paper-toolbar.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:33] \"GET /paper-checkbox/paper-checkbox.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:33] \"GET /iron-icons/iron-icons.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:33] \"GET /paper-header-panel/paper-header-panel.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:33] \"GET /paper-button/paper-button.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:33] \"GET /paper-dialog-behavior/paper-dialog-behavior.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:33] \"GET /neon-animation/neon-animation-runner-behavior.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:33] \"GET /paper-dialog-behavior/paper-dialog-shared-styles.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:33] \"GET /iron-flex-layout/iron-flex-layout.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:33] \"GET /iron-menu-behavior/iron-menubar-behavior.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:33] \"GET /iron-icon/iron-icon.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:33] \"GET /iron-resizable-behavior/iron-resizable-behavior.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:34] \"GET /paper-icon-button/paper-icon-button.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:34] \"GET /paper-styles/color.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:34] \"GET /paper-tabs/paper-tabs-icons.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:34] \"GET /paper-tabs/paper-tab.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:34] \"GET /paper-styles/default-theme.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:34] \"GET /paper-behaviors/paper-checked-element-behavior.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:34] \"GET /paper-styles/typography.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:34] \"GET /paper-material/paper-material.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:34] \"GET /iron-iconset-svg/iron-iconset-svg.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:34] \"GET /paper-ripple/paper-ripple.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:34] \"GET /paper-behaviors/paper-button-behavior.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:34] \"GET /iron-meta/iron-meta.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:34] \"GET /neon-animation/neon-animatable-behavior.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:34] \"GET /iron-overlay-behavior/iron-overlay-behavior.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:34] \"GET /paper-styles/shadow.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:34] \"GET /polymer/polymer-mini.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:34] \"GET /paper-behaviors/paper-inky-focus-behavior.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:34] \"GET /iron-menu-behavior/iron-menu-behavior.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:34] \"GET /iron-behaviors/iron-button-state.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:34] \"GET /iron-behaviors/iron-control-state.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:34] \"GET /paper-behaviors/paper-ripple-behavior.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:34] \"GET /iron-checked-element-behavior/iron-checked-element-behavior.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:34] \"GET /font-roboto/roboto.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:34] \"GET /iron-a11y-keys-behavior/iron-a11y-keys-behavior.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:34] \"GET /paper-material/paper-material-shared-styles.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:34] \"GET /iron-fit-behavior/iron-fit-behavior.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:34] \"GET /neon-animation/animations/opaque-animation.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:34] \"GET /iron-overlay-behavior/iron-overlay-manager.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:34] \"GET /polymer/polymer-micro.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:34] \"GET /iron-selector/iron-multi-selectable.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:34] \"GET /iron-form-element-behavior/iron-form-element-behavior.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:34] \"GET /iron-validatable-behavior/iron-validatable-behavior.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:34] \"GET /neon-animation/neon-animation-behavior.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:34] \"GET /neon-animation/web-animations.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:34] \"GET /iron-selector/iron-selectable.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:34] \"GET /iron-overlay-behavior/iron-overlay-backdrop.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:34] \"GET /web-animations-js/web-animations-next-lite.min.js HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:34] \"GET /iron-selector/iron-selection.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:34] \"GET /lodash/lodash.min.js HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:34] \"GET /paper-slider/paper-slider.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:34] \"GET /paper-input/paper-input.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:34] \"GET /paper-progress/paper-progress.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:34] \"GET /iron-range-behavior/iron-range-behavior.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:34] \"GET /paper-input/paper-input-behavior.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:34] \"GET /paper-input/paper-input-char-counter.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:34] \"GET /iron-input/iron-input.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:34] \"GET /paper-input/paper-input-container.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:35] \"GET /paper-input/paper-input-error.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:35] \"GET /iron-a11y-announcer/iron-a11y-announcer.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:35] \"GET /paper-input/paper-input-addon-behavior.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:35] \"GET /d3/d3.js HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:35] \"GET /paper-styles/paper-styles.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:35] \"GET /iron-flex-layout/classes/iron-flex-layout.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:35] \"GET /iron-flex-layout/classes/iron-shadow-flex-layout.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:35] \"GET /paper-menu/paper-menu.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:35] \"GET /paper-dropdown-menu/paper-dropdown-menu.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:35] \"GET /paper-item/paper-item.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:35] \"GET /paper-menu/paper-menu-shared-styles.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:36] \"GET /paper-item/paper-item-behavior.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:36] \"GET /paper-dropdown-menu/paper-dropdown-menu-shared-styles.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:36] \"GET /paper-dropdown-menu/paper-dropdown-menu-icons.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:36] \"GET /paper-menu-button/paper-menu-button.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:36] \"GET /paper-item/paper-item-shared-styles.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:36] \"GET /neon-animation/animations/fade-out-animation.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:36] \"GET /neon-animation/animations/fade-in-animation.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:36] \"GET /paper-menu-button/paper-menu-button-animations.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:36] \"GET /iron-dropdown/iron-dropdown.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:36] \"GET /iron-dropdown/iron-dropdown-scroll-manager.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:36] \"GET /iron-collapse/iron-collapse.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:36] \"GET /plottable/plottable.js HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:36] \"GET /plottable/plottable.css HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:36] \"GET /paper-toggle-button/paper-toggle-button.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:37] \"GET /graphlib/dist/graphlib.core.js HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:37] \"GET /dagre/dist/dagre.core.js HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:37] \"GET /paper-item/all-imports.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:37] \"GET /iron-list/iron-list.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:37] \"GET /paper-item/paper-icon-item.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:37] \"GET /paper-item/paper-item-body.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:37] \"GET /paper-radio-group/paper-radio-group.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:37] \"GET /paper-tooltip/paper-tooltip.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:37] \"GET /paper-radio-button/paper-radio-button.html HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:37] \"GET /data/runs HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:38] \"GET /data/runs HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:41] \"GET /data/runs HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:41] \"GET /data/runs HTTP/1.1\" 200 -\n",
      "192.168.99.1 - - [11/Nov/2016 02:35:42] \"GET /data/graph?run=.&limit_attr_size=1024&large_attrs_key=_too_large_attrs HTTP/1.1\" 200 -\n",
      "^CTraceback (most recent call last):\n",
      "  File \"/usr/local/bin/tensorboard\", line 11, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard/tensorboard.py\", line 151, in main\n",
      "    tb_server.serve_forever()\n",
      "  File \"/usr/lib/python2.7/SocketServer.py\", line 236, in serve_forever\n",
      "    poll_interval)\n",
      "  File \"/usr/lib/python2.7/SocketServer.py\", line 155, in _eintr_retry\n",
      "    return func(*args)\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir tf_summaries --port 6006\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above will load your implementation, construct the graph, and write a logdir for TensorBoard. You can bring up TensorBoard with:\n",
    "```\n",
    "tensorboard --logdir tf_summaries --port 6006\n",
    "```\n",
    "As usual, check http://localhost:6006/#graphs to inspect your implementation. Remember, judicious use of `tf.name_scope()` and/or `tf.variable_scope()` will greatly improve the visualization, and make code easier to debug."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Training your RNNLM (5 points)\n",
    "\n",
    "We'll give you data loader functions in **`utils.py`**. They work similarly to the loaders in the Week 4 notebook.\n",
    "\n",
    "Particularly, `utils.batch_generator` will return an iterator that yields minibatches in the correct format. Batches will be of size `[batch_size, max_time]`, and consecutive batches will line up along rows so that the final state $h^{\\text{final}}$ of one batch can be used as the initial state $h^{\\text{init}}$ for the next.\n",
    "\n",
    "For example, using a toy corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a ['<s>' 'Mary' 'had' 'a' 'little' 'lamb' '.' '<s>' 'The' 'lamb' 'was'\n",
      " 'white' 'as' 'snow' '.' '<s>']\n",
      "Input words:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w_0</th>\n",
       "      <th>w_1</th>\n",
       "      <th>w_2</th>\n",
       "      <th>w_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>Mary</td>\n",
       "      <td>had</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>The</td>\n",
       "      <td>lamb</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   w_0   w_1   w_2  w_3\n",
       "0  <s>  Mary   had    a\n",
       "1  <s>   The  lamb  was"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w_0</th>\n",
       "      <th>w_1</th>\n",
       "      <th>w_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>little</td>\n",
       "      <td>lamb</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>white</td>\n",
       "      <td>as</td>\n",
       "      <td>snow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      w_0   w_1   w_2\n",
       "0  little  lamb     .\n",
       "1   white    as  snow"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target words:\n",
      "b ['<s>' 'Mary' 'had' 'a' 'little' 'lamb' '.' '<s>' 'The' 'lamb' 'was'\n",
      " 'white' 'as' 'snow' '.' '<s>']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_0</th>\n",
       "      <th>y_1</th>\n",
       "      <th>y_2</th>\n",
       "      <th>y_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mary</td>\n",
       "      <td>had</td>\n",
       "      <td>a</td>\n",
       "      <td>little</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The</td>\n",
       "      <td>lamb</td>\n",
       "      <td>was</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    y_0   y_1  y_2     y_3\n",
       "0  Mary   had    a  little\n",
       "1   The  lamb  was   white"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_0</th>\n",
       "      <th>y_1</th>\n",
       "      <th>y_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lamb</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>as</td>\n",
       "      <td>snow</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    y_0   y_1  y_2\n",
       "0  lamb     .  <s>\n",
       "1    as  snow    ."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "toy_corpus = \"<s> Mary had a little lamb . <s> The lamb was white as snow . <s>\"\n",
    "toy_corpus = np.array(toy_corpus.split())\n",
    "print \"a\", toy_corpus\n",
    "print \"Input words:\"\n",
    "bi = utils.batch_generator(toy_corpus, batch_size=2, max_time=4)\n",
    "for i, (w,y) in enumerate(bi):\n",
    "  utils.pretty_print_matrix(w, cols=[\"w_%d\" % d for d in range(w.shape[1])], dtype=object)\n",
    "\n",
    "print \"Target words:\"\n",
    "bi = utils.batch_generator(toy_corpus, batch_size=2, max_time=4)\n",
    "print \"b\", toy_corpus\n",
    "for i, (w,y) in enumerate(bi):\n",
    "  utils.pretty_print_matrix(y, cols=[\"y_%d\" % d for d in range(w.shape[1])], dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the data we feed to our model will be word indices, but the shape will be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implement the `run_epoch` function\n",
    "We've given you some starter code for logging progress; fill this in with actual call(s) to `session.run` with the appropriate arguments to run a training step. \n",
    "\n",
    "Be sure to handle the initial state properly at the beginning of an epoch, and remember to carry over the final state from each batch and use it as the initial state for the next.\n",
    "\n",
    "**Note:** we provide a `train=True` flag to enable train mode. If `train=False`, this function can also be used for scoring the dataset - see `score_dataset()` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epoch(lm, session, batch_iterator, train=False,\n",
    "              verbose=False, tick_s=10, \n",
    "              keep_prob=1.0, learning_rate=0.1):\n",
    "  start_time = time.time()\n",
    "  tick_time = start_time  # for showing status\n",
    "  total_cost = 0.0  # total cost, summed over all words\n",
    "  total_words = 0\n",
    "\n",
    "  if train:\n",
    "    train_op = lm.train_step_\n",
    "    keep_prob = keep_prob\n",
    "    loss = lm.train_loss_\n",
    "  else:\n",
    "    train_op = tf.no_op()\n",
    "    keep_prob = 1.0  # no dropout at test time\n",
    "    loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "\n",
    "  for i, (w, y) in enumerate(batch_iterator):\n",
    "    cost = 0.0\n",
    "    #### YOUR CODE HERE ####\n",
    "    \n",
    "    # At first batch in epoch, get a clean intitial state\n",
    "    if i == 0:\n",
    "      h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "    cost = session.run(cost_)\n",
    "    \n",
    "    \n",
    "    #### END(YOUR CODE) ####\n",
    "    total_cost += cost\n",
    "    total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "    ##\n",
    "    # Print average loss-so-far for epoch\n",
    "    # If using train_loss_, this may be an underestimate.\n",
    "    if verbose and (time.time() - tick_time >= tick_s):\n",
    "      avg_cost = total_cost / total_words\n",
    "      avg_wps = total_words / (time.time() - start_time)\n",
    "      print \"[batch %d]: seen %d words at %d wps, loss = %.3f\" % (i,\n",
    "          total_words, avg_wps, avg_cost)\n",
    "      tick_time = time.time()  # reset time ticker\n",
    "\n",
    "  return total_cost / total_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Run Training\n",
    "\n",
    "We'll give you the outline of the training procedure, but you'll need to fill in a call to your `run_epoch` function. \n",
    "\n",
    "At the end of training, we use a `tf.train.Saver` to save a copy of the model to `./tf_saved/rnnlm_trained`. You'll want to load this from disk to work on later parts of the assignment; see **part (d)** for an example of how this is done.\n",
    "\n",
    "#### Tuning Hyperparameters\n",
    "With a sampled softmax loss, the default hyperparameters should train 5 epochs in under 10 minutes on a 16-core Google Cloud Compute instance, and reach a training set perplexity below 200.\n",
    "\n",
    "However, it's possible to do significantly better. Try experimenting with multiple RNN layers (`num_layers` > 1) or a larger hidden state - though you may also need to adjust the learning rate and number of epochs for a larger model.\n",
    "\n",
    "You can also experiment with a larger vocabulary. This will look worse for perplexity, but will be a better model overall as it won't treat so many words as `<unk>`.\n",
    "\n",
    "#### Submitting your model\n",
    "You should submit your trained model along with the assignment. Do:\n",
    "```\n",
    "git add tf_saved/rnnlm_trained tf_saved/rnnlm_trained.meta\n",
    "git commit -m \"Adding trained model.\"\n",
    "```\n",
    "Unless you train a very large model, these files should be < 50 MB and no problem for git to handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "V = 10000\n",
    "vocab, train_ids, test_ids = utils.load_corpus(\"brown\", split=0.8, V=V, shuffle=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "max_time = 20\n",
    "batch_size = 50\n",
    "learning_rate = 0.5\n",
    "keep_prob = 1.0\n",
    "num_epochs = 5\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(V=V, \n",
    "                    H=100, \n",
    "                    num_layers=1)\n",
    "\n",
    "trained_filename = 'tf_saved/rnnlm_trained'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def score_dataset(lm, session, ids, name=\"Data\"):\n",
    "  bi = utils.batch_generator(ids, batch_size=100, max_time=100)\n",
    "  cost = run_epoch(lm, session, bi, \n",
    "                   learning_rate=1.0, keep_prob=1.0, \n",
    "                   train=False, verbose=False, tick_s=3600)\n",
    "  print \"%s: avg. loss: %.03f  (perplexity: %.02f)\" % (name, cost, np.exp(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Will print status every this many seconds\n",
    "print_interval = 5\n",
    "\n",
    "# Clear old log directory\n",
    "shutil.rmtree(\"tf_summaries\", ignore_errors=True)\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session() as session:\n",
    "  # Seed RNG for repeatability\n",
    "  tf.set_random_seed(42)\n",
    "  \n",
    "  with tf.variable_scope(\"model\", reuse=None):\n",
    "    lm = rnnlm.RNNLM(**model_params)\n",
    "    lm.BuildCoreGraph()\n",
    "    lm.BuildTrainGraph()\n",
    "  \n",
    "  session.run(tf.initialize_all_variables())\n",
    "  saver = tf.train.Saver()\n",
    "  \n",
    "  for epoch in xrange(1,num_epochs+1):\n",
    "    t0_epoch = time.time()\n",
    "    bi = utils.batch_generator(train_ids, batch_size, max_time)\n",
    "    print \"[epoch %d] Starting epoch %d\" % (epoch, epoch)\n",
    "    #### YOUR CODE HERE ####\n",
    "    \n",
    "    # Run a training epoch.\n",
    "    \n",
    "    #### END(YOUR CODE) ####\n",
    "    print \"[epoch %d] Completed in %s\" % (epoch, utils.pretty_timedelta(since=t0_epoch))\n",
    "    \n",
    "    ##\n",
    "    # score_dataset will run a forward pass over the entire dataset\n",
    "    # and report perplexity scores. This can be slow (around 1/2 to \n",
    "    # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "    # to speed up training on a slow machine. Be sure to run it at the \n",
    "    # end to evaluate your score.\n",
    "    print (\"[epoch %d]\" % epoch),\n",
    "    score_dataset(lm, session, train_ids, name=\"Train set\")\n",
    "    print (\"[epoch %d]\" % epoch),\n",
    "    score_dataset(lm, session, test_ids, name=\"Test set\")\n",
    "    print \"\"\n",
    "    \n",
    "    # Save a checkpoint\n",
    "    saver.save(session, 'tf_saved/rnnlm', global_step=epoch)\n",
    "    \n",
    "  # Save final model\n",
    "  saver.save(session, trained_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) Sampling Sentences (5 points)\n",
    "\n",
    "If you didn't already in **part (b)**, implement the `BuildSamplerGraph()` method in `rnnlm.py` See the function docstring for more information.\n",
    "\n",
    "#### Implement the `sample_step()` method below (5 points)\n",
    "This should access the Tensors you create in `BuildSamplerGraph()`. Given an input batch and initial states, it should return a vector of shape `[batch_size,]` containing sampled indices for the next word of each batch sequence.\n",
    "\n",
    "Run the method using the provided code to generate 10 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_step(lm, session, input_w, initial_h):\n",
    "  \"\"\"Run a single RNN step and return sampled predictions.\n",
    "  \n",
    "  Args:\n",
    "    lm : rnnlm.RNNLM\n",
    "    session: tf.Session\n",
    "    input_w : [batch_size] list of indices\n",
    "    initial_h : [batch_size, hidden_dims]\n",
    "  \n",
    "  Returns:\n",
    "    final_h : final hidden state, compatible with initial_h\n",
    "    samples : [batch_size, 1] vector of indices\n",
    "  \"\"\"\n",
    "  #### YOUR CODE HERE ####\n",
    "  # Reshape input to column vector\n",
    "  input_w = np.array(input_w, dtype=np.int32).reshape([-1,1])\n",
    "  \n",
    "  # Run sample ops\n",
    "  \n",
    "  #### END(YOUR CODE) ####\n",
    "  return final_h, samples[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Same as above, but as a batch\n",
    "max_steps = 20\n",
    "num_samples = 10\n",
    "random_seed = 42\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session() as session:\n",
    "  # Seed RNG for repeatability\n",
    "  tf.set_random_seed(random_seed)\n",
    "\n",
    "  with tf.variable_scope(\"model\", reuse=None):\n",
    "    lm = rnnlm.RNNLM(**model_params)\n",
    "    lm.BuildCoreGraph()\n",
    "    lm.BuildSamplerGraph()\n",
    "\n",
    "  # Load the trained model\n",
    "  saver = tf.train.Saver()\n",
    "  saver.restore(session, trained_filename)\n",
    "\n",
    "  # Make initial state for a batch with batch_size = num_samples\n",
    "  w = np.repeat([[vocab.START_ID]], num_samples, axis=0)\n",
    "  h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "  # We'll take one step for each sequence on each iteration \n",
    "  for i in xrange(max_steps):\n",
    "    state, y = sample_step(lm, session, w[:,-1:], h)\n",
    "    w = np.hstack((w,y))\n",
    "\n",
    "  # Print generated sentences\n",
    "  for row in w:\n",
    "    for i, word_id in enumerate(row):\n",
    "      print vocab.id_to_word[word_id],\n",
    "        if (i != 0) and (word_id == vocab.START_ID):\n",
    "          break\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## (e) Linguistic Properties (5 points)\n",
    "\n",
    "Now that we've trained our RNNLM, let's test a few properties of the model to see how well it learns linguistic phenomena. We'll do this with a scoring task: given two or more test sentences, our model should score the more plausible (or more correct) sentence with a higher log-probability.\n",
    "\n",
    "We'll define a scoring function to help us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_seq(lm, session, seq, vocab):\n",
    "  \"\"\"Score a sequence of words. Returns total log-probability.\"\"\"\n",
    "  padded_ids = vocab.words_to_ids(utils.canonicalize_words([\"<s>\"] + seq, \n",
    "                                                           wordset=vocab.word_to_id))\n",
    "  w = np.reshape(padded_ids[:-1], [1,-1])\n",
    "  y = np.reshape(padded_ids[1:],  [1,-1])\n",
    "  h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "  feed_dict = {lm.input_w_:w,\n",
    "               lm.target_y_:y,\n",
    "               lm.initial_h_:h,\n",
    "               lm.dropout_keep_prob_: 1.0}\n",
    "  # Return log(P(seq)) = -1*loss\n",
    "  return -1*session.run(lm.loss_, feed_dict)\n",
    "\n",
    "def load_and_score(inputs, sort=False):\n",
    "  \"\"\"Load the trained model and score the given words.\"\"\"\n",
    "  with tf.Graph().as_default(), tf.Session() as session:  \n",
    "    with tf.variable_scope(\"model\", reuse=None):\n",
    "      lm = rnnlm.RNNLM(**model_params)\n",
    "      lm.BuildCoreGraph()\n",
    "        \n",
    "    # Load the trained model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(session, trained_filename)\n",
    "  \n",
    "    if isinstance(inputs[0], str) or isinstance(inputs[0], unicode):\n",
    "      inputs = [inputs]\n",
    "\n",
    "    # Actually run scoring\n",
    "    results = []\n",
    "    for words in inputs:\n",
    "      score = score_seq(lm, session, words, vocab)\n",
    "      results.append((score, words))\n",
    "    \n",
    "    # Sort if requested\n",
    "    if sort: results = sorted(results, reverse=True)\n",
    "    \n",
    "    # Print results\n",
    "    for score, words in results:\n",
    "      print \"\\\"%s\\\" : %.02f\" % (\" \".join(words), score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sents = [\"once upon a time\",\n",
    "         \"the quick brown fox jumps over the lazy dog\"]\n",
    "load_and_score([s.split() for s in sents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Number agreement\n",
    "\n",
    "Compare **\"the boy and the girl [are/is]\"**. Which is more plausible according to your model?\n",
    "\n",
    "If your model doesn't order them correctly (*this is OK*), why do you think that might be? (answer in cell below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "\n",
    "\n",
    "#### END(YOUR CODE) ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer to part 1. question(s)\n",
    "\n",
    "*Answer to above question(s).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Type/semantic agreement\n",
    "\n",
    "Compare:\n",
    "- **\"peanuts are my favorite kind of [nut/vegetable]\"**\n",
    "- **\"when I'm hungry I really prefer to [eat/drink]\"**\n",
    "\n",
    "Of each pair, which is more plausible according to your model?\n",
    "\n",
    "How would you expect a 3-gram language model to perform at this example? How about a 5-gram model? (answer in cell below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### END(YOUR CODE) ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer to part 2. question(s)\n",
    "\n",
    "*Answer to above question(s).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Adjective ordering (just for fun)\n",
    "\n",
    "Let's repeat the exercise from Week 2:\n",
    "\n",
    "![Adjective Order](adjective_order.jpg)\n",
    "*source: https://twitter.com/MattAndersonBBC/status/772002757222002688?lang=en*\n",
    "\n",
    "We'll consider a toy example (literally), and consider all possible adjective permutations.\n",
    "\n",
    "Note that this is somewhat sensitive to training, and even a good language model might not get it all correct. Why might the NN fail, if the trigram model from week2 was able to solve it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prefix = \"I have lots of\".split()\n",
    "noun = \"toys\"\n",
    "adjectives = [\"square\", \"green\", \"plastic\"]\n",
    "inputs = []\n",
    "for adjs in itertools.permutations(adjectives):\n",
    "  words = prefix + list(adjs) + [noun]\n",
    "  inputs.append(words)\n",
    "    \n",
    "load_and_score(inputs, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
