{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 3: Clusters and Distributions\n",
    "\n",
    "In this week's section, we'll explore word representations. Particularly, we'll focus on SVD-based embeddings; you'll have a chance to implement a more sophisticated technique and explore neural embeddings in Assignment 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vocabulary\n",
      "/usr/local/lib/python2.7/dist-packages/pip/_vendor/requests/packages/urllib3/util/ssl_.py:318: SNIMissingWarning: An HTTPS request has been made, but the SNI (Subject Name Indication) extension to TLS is not available on this platform. This may cause the server to present an incorrect TLS certificate, which can cause validation failures. You can upgrade to a newer version of Python to solve this. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#snimissingwarning.\n",
      "  SNIMissingWarning\n",
      "/usr/local/lib/python2.7/dist-packages/pip/_vendor/requests/packages/urllib3/util/ssl_.py:122: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. You can upgrade to a newer version of Python to solve this. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning.\n",
      "  InsecurePlatformWarning\n",
      "  Downloading Vocabulary-0.0.5.tar.gz\n",
      "Collecting requests==2.8.1 (from vocabulary)\n",
      "  Downloading requests-2.8.1-py2.py3-none-any.whl (497kB)\n",
      "\u001b[K    100% |################################| 501kB 2.1MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: vocabulary\n",
      "  Running setup.py bdist_wheel for vocabulary ... \u001b[?25l-\b \bdone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/b3/ff/55/dd1c0ca267f4a5971b0a0ab75f2a6e091eb13395e0096cf51b\n",
      "Successfully built vocabulary\n",
      "Installing collected packages: requests, vocabulary\n",
      "  Found existing installation: requests 2.11.1\n",
      "    Uninstalling requests-2.11.1:\n",
      "      Successfully uninstalled requests-2.11.1\n",
      "Successfully installed requests-2.8.1 vocabulary-0.0.5\n",
      "Requirement already up-to-date: scipy in /usr/local/lib/python2.7/dist-packages\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'vocabulary' from 'vocabulary.pyc'>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install --upgrade pandas\n",
    "#!pip install --upgrade itertools\n",
    "#!pip install --upgrade collections\n",
    "!pip install --upgrade vocabulary\n",
    "import os, sys, re, json, time\n",
    "import itertools\n",
    "import collections\n",
    "from IPython.display import display\n",
    "\n",
    "# NumPy and SciPy for matrix ops\n",
    "import numpy as np\n",
    "!pip install --upgrade scipy\n",
    "import scipy.sparse\n",
    "\n",
    "# Pandas because pandas are awesome\n",
    "import pandas as pd\n",
    "# Set pandas floating point display\n",
    "pd.set_option('float_format', lambda f: \"{0:.04f}\".format(f))\n",
    "\n",
    "# NLTK for NLP utils\n",
    "import nltk\n",
    "\n",
    "import data_utils\n",
    "reload(data_utils)\n",
    "import vocabulary\n",
    "reload(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def flatten(list_of_lists):\n",
    "    return itertools.chain.from_iterable(list_of_lists)\n",
    "\n",
    "def pretty_print_matrix(M, rows=None, cols=None, dtype=float):\n",
    "    display(pd.DataFrame(M, index=rows, columns=cols, dtype=dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the `Vocabulary` class again, this time to map words to numerical indices. These will be the row indices of our embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common unigrams:\n",
      "\"the\": 69971\n",
      "\",\": 58334\n",
      "\".\": 49346\n",
      "\"of\": 36412\n",
      "\"and\": 28853\n",
      "\"to\": 26158\n",
      "\"a\": 23195\n",
      "\"in\": 21337\n",
      "\"that\": 10594\n",
      "\"is\": 10109\n"
     ]
    }
   ],
   "source": [
    "corpus = nltk.corpus.brown\n",
    "vocab_size = 10000\n",
    "\n",
    "token_feed = (data_utils.canonicalize_word(w) for w in corpus.words())\n",
    "vocab = vocabulary.Vocabulary(token_feed, vocab_size)\n",
    "\n",
    "print \"Most common unigrams:\"\n",
    "print \"\\n\".join(\"\\\"%s\\\": %d\" % kv \n",
    "                for kv in vocab.unigram_counts.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-occurrence matrix\n",
    "\n",
    "We want to build a co-occurrence matrix $C$, where $C_{ij}$ is the number of times word $i$ occurs in the same context as word $j$.\n",
    "\n",
    "This matrix will be of size `vocab_size` x `vocab_size`, which is quite large - we'd run into the same problem we had with a bigram LM if we represented it explicitly.\n",
    "\n",
    "Instead, we'll use a **sparse matrix** (`scipy.sparse`). This only stores the non-zero elements, but still supports some of the usual matrix operations like slicing, multiplication, and - most importantly - the SVD. \n",
    "\n",
    "See [the scipy.sparse documentation](http://docs.scipy.org/doc/scipy/reference/sparse.html) for more info on how these work; we'll chiefly use the DOK, COO, and CSR formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed-Size Window\n",
    "It's simplest to implement this for a fixed-size window. Naively, we can just go through the entire corpus, and at each position `i` add a count for the words at positions `i-2, i-1, i+1, i+2`, etc.\n",
    "\n",
    "Turns out this is really slow in Python, so we'll re-order things: we'll count all pairs `(i,i-2)`, `(i,i-1)`, etc. and add the counts up at the end.\n",
    "\n",
    "Note that pairs are symmetric: so at the same time we handle `(i,i+k)`, we'll also add an entry for `(i+k,i)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cooccurrence_matrix(ids, vocab, window_size=2):\n",
    "    # We'll use this as an \"accumulator\" matrix\n",
    "    C = scipy.sparse.csc_matrix((vocab.size, vocab.size), \n",
    "                                dtype=np.float32)\n",
    "    \n",
    "    for k in range(1, window_size+1):\n",
    "        if k == 0: continue  # don't count (i,i)\n",
    "        print u\"Counting pairs (i, i \\u00B1 %d) ...\" % k\n",
    "        # Consider words k positions ahead\n",
    "        i = ids[:-k] # current word\n",
    "        j = ids[k:]  # k words ahead\n",
    "        # Construct a COO matrix: values, indices\n",
    "        # scipy.sparse will add up duplicates for us\n",
    "        data = (np.ones_like(i), (i,j))\n",
    "        Ck_plus = scipy.sparse.coo_matrix(data, shape=C.shape, dtype=np.float32)\n",
    "        Ck_plus = scipy.sparse.csc_matrix(Ck_plus)\n",
    "        Ck_minus = Ck_plus.T  # Consider k words behind\n",
    "        C += Ck_plus + Ck_minus\n",
    "\n",
    "    print \"Co-occurrence matrix: %d words x %d words\" % (C.shape)\n",
    "    print \"  %.02g nonzero elements\" % (C.nnz)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a toy corpus to see how this works. With a window of 1, we should see co-occurrence counts for each pair of neighboring words:  \n",
    "`(<s>, nlp)`,  \n",
    "`(nlp, class)`,  \n",
    "`(class, is)`,  \n",
    "and so on - as well as their reversed versions (remember, C is symmetric!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting pairs (i, i ± 1) ...\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-4533ebf8b3e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# Here's the important part\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mtoy_C\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcooccurrence_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoy_vocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mtoy_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoy_vocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mordered_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-d2513b12f273>\u001b[0m in \u001b[0;36mcooccurrence_matrix\u001b[1;34m(ids, vocab, window_size)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mCk_plus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsc_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCk_plus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mCk_minus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCk_plus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m  \u001b[1;31m# Consider k words behind\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mC\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mCk_plus\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mCk_minus\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Co-occurrence matrix: %d words x %d words\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/dist-packages/scipy/sparse/base.pyc\u001b[0m in \u001b[0;36m__iadd__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iadd__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 387\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    388\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__isub__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "toy_corpus = [\n",
    "    \"nlp class is awesome\",\n",
    "    \"nlp class is fun\"\n",
    "]\n",
    "\n",
    "words = list(flatten([s.split() for s in toy_corpus]))\n",
    "toy_vocab = vocabulary.Vocabulary(words)\n",
    "# sentence_to_ids adds \"<s>\" and \"</s>\"\n",
    "ids = list(flatten(toy_vocab.sentence_to_ids(s.split()) \n",
    "                   for s in toy_corpus))\n",
    "\n",
    "# Here's the important part\n",
    "toy_C = cooccurrence_matrix(ids, toy_vocab, window_size=1)\n",
    "\n",
    "toy_labels = toy_vocab.ordered_words()\n",
    "pretty_print_matrix(toy_C.toarray(), rows=toy_labels, \n",
    "                    cols=toy_labels, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting pairs (i, i ± 1) ...\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-caf6055a8b10>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mt0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcooccurrence_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Constructed C in %s sec\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpretty_timedelta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mt0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-d2513b12f273>\u001b[0m in \u001b[0;36mcooccurrence_matrix\u001b[1;34m(ids, vocab, window_size)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mCk_plus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsc_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCk_plus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mCk_minus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCk_plus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m  \u001b[1;31m# Consider k words behind\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mC\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mCk_plus\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mCk_minus\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Co-occurrence matrix: %d words x %d words\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/dist-packages/scipy/sparse/base.pyc\u001b[0m in \u001b[0;36m__iadd__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iadd__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 387\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    388\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__isub__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Canonicalize words and convert to numerical IDs\n",
    "words = data_utils.canonicalize_words(corpus.words(), \n",
    "                                      wordset=vocab.word_to_id)\n",
    "ids = np.array(vocab.words_to_ids(words))\n",
    "\n",
    "t0 = time.time()\n",
    "C = cooccurrence_matrix(ids, vocab)\n",
    "print \"Constructed C in %s sec\" % data_utils.pretty_timedelta(since=t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Context\n",
    "\n",
    "Naively, we could do this by using something like:\n",
    "```\n",
    "for i in sentence_ids[:-1]:\n",
    "  for j in sentence_ids[1:]:\n",
    "     C[i,j] += 1\n",
    "```\n",
    "But unfortunately, Python is slow, and for long sentences (say, 40 tokens), this can take a loooong time.\n",
    "\n",
    "However, there's a clever trick. We can compute a matrix $M_{ik}$ where rows $i$ are word indices and columns $k$ are sentence indices. Assuming each word appears only once in a sentence, then we have:\n",
    "\n",
    "$$ C_{ij} = \\sum_{k} \\mathbf{1}[w_i \\in \\text{sentence}\\ k] \\cdot \\mathbf{1}[w_j \\in \\text{sentence}\\ k] = \\sum_{k} M_{ik} M_{jk} = (MM^T)_{ij} $$\n",
    "\n",
    "So we can compute $C_{ij}$ easily with matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cooccurrence_matrix_sentences(sentence_ids, vocab):\n",
    "    M = scipy.sparse.dok_matrix((vocab.size, len(sentence_ids)), \n",
    "                                dtype=np.int32)\n",
    "    \n",
    "    for j,ids in enumerate(sentence_ids):\n",
    "        for i in ids:\n",
    "            M[i,j] += 1\n",
    "    \n",
    "    M = scipy.sparse.csr_matrix(M)\n",
    "    # Correction for multiple occurrences of words\n",
    "    diag_corr = scipy.sparse.dia_matrix((np.ravel(M.sum(1)), [0]),\n",
    "                                        shape=(vocab.size, vocab.size))\n",
    "    C = M.dot(M.T) - diag_corr\n",
    "    print \"Co-occurrence matrix: %d words x %d words\" % (C.shape)\n",
    "    print \"  %.02g nonzero elements\" % (C.nnz)\n",
    "    return C\n",
    "    \n",
    "sentence_ids = [vocab.words_to_ids(data_utils.canonicalize_words(s)) \n",
    "                for s in corpus.sents()]\n",
    "t0 = time.time()\n",
    "C_sentence = cooccurrence_matrix_sentences(sentence_ids, vocab)\n",
    "print \"Constructed C in %s\" % data_utils.pretty_timedelta(since=t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** It's also common to do SVD directly on the word-sentence matrix M, particularly where instead of sentences we use paragraphs or whole documents. This is known as Latent Semantic Analysis, or LSA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this on our toy corpus again. You should notice a somewhat denser matrix this time: every pair of words are represented, not just the adjacent ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence_ids = [toy_vocab.sentence_to_ids(s.split()) \n",
    "                for s in toy_corpus]\n",
    "\n",
    "toy_C_s = cooccurrence_matrix_sentences(sentence_ids, toy_vocab)\n",
    "\n",
    "pretty_print_matrix(toy_C_s.toarray(), rows=toy_labels, \n",
    "                    cols=toy_labels, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Word Vectors\n",
    "\n",
    "In order to go from our co-occurrence matrix to word vectors, we need to do two things:\n",
    "\n",
    "- First, convert to **PPMI** to reduce the impact of common words.\n",
    "- Compute the SVD, and extract our vectors.\n",
    "\n",
    "PPMI stands for Positive [Pointwise Mutual Information](https://en.wikipedia.org/wiki/Pointwise_mutual_information). PMI is a generalization of the idea of correlation, but for arbitrary variables. Here, we're interested in the correlation between word $i$ and word $j$, where we take the samples to be all the word-word pairs in our corpus.  \n",
    "Positive just means we'll truncate at zero: $\\text{PPMI}(i,j) = \\max(0, \\text{PMI}(i,j))$\n",
    "\n",
    "We'll apply PPMI as a transformation of our counts matrix. First, compute probabilities:\n",
    "$$ P(i,j) = \\frac{C(i,j)}{\\sum_{k,l} C(k,l)} = \\frac{C_{ij}}{Z}$$\n",
    "$$ P(i) = \\frac{\\sum_{k} C(i,k)}{\\sum_{k,l} C(k,l)} = \\frac{Z_i}{Z}$$\n",
    "\n",
    "Then compute PPMI:\n",
    "$$ \\text{PMI}(i,j) = \\log \\frac{P(i,j)}{P(i)P(j)} = \\log \\frac{C_{ij} \\cdot Z}{Z_i \\cdot Z_j} $$\n",
    "$$\\text{PPMI}(i,j) = \\max(0, \\text{PMI}(i,j))$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PPMI(C):\n",
    "    # Expect C to be a CSC matrix\n",
    "    Z = float(C.sum())  # total counts\n",
    "    Zc = np.array(C.sum(axis=0), dtype=np.float64).flatten() # sum each column (along rows)\n",
    "    Zr = np.array(C.sum(axis=1), dtype=np.float64).flatten() # sum each row (along columns)\n",
    "    \n",
    "    # Get indices of relevant elements\n",
    "    ii, jj = C.nonzero()  # row, column indices\n",
    "    Cij = np.array(C[ii,jj], dtype=np.float64).flatten()\n",
    "    pmi = np.log(Cij * Z / (Zr[ii] * Zc[jj]))\n",
    "    ppmi = np.maximum(0, pmi)  # take positive only\n",
    "    ret = scipy.sparse.csc_matrix((ppmi, (ii,jj)), shape=C.shape,\n",
    "                                  dtype=np.float64)\n",
    "    ret.eliminate_zeros()  # remove zeros\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to compute the SVD, we can just use [`sklearn.decomposition.TruncatedSVD`](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "def SVD(X, k=100):\n",
    "    transformer = TruncatedSVD(n_components=k, random_state=1)\n",
    "    Wv = transformer.fit_transform(X)\n",
    "    # Normalize to unit length\n",
    "    Wv = Wv / np.linalg.norm(Wv, axis=1).reshape([-1,1])\n",
    "    return Wv, transformer.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "X = PPMI(C)\n",
    "print \"Computed PPMI in %s\" % data_utils.pretty_timedelta(since=t0)\n",
    "\n",
    "t0 = time.time()\n",
    "Wv, svs = SVD(X)\n",
    "print \"Computed SVD in %s\" % data_utils.pretty_timedelta(since=t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what this does on our toy corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pretty_print_matrix(PPMI(toy_C).toarray(), rows=toy_labels, \n",
    "                    cols=toy_labels, dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we take the SVD, we'll get the words represented by row vectors. Note that \"fun\" and \"awesome\" get the same representation, since they're interchangable in our (tiny) corpus. How do their embedding vectors relate to the one for \"class\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = 3\n",
    "pretty_print_matrix(SVD(PPMI(toy_C), k=k)[0], rows=toy_labels, \n",
    "                    cols=range(k), dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Word Vectors\n",
    "\n",
    "We've included some code that will generate an interactive visualization of the word cloud. You can use the mouse to pan and zoom, and you can pass a dict of colors to highlight or color-code words.\n",
    "\n",
    "It's implemented using the [Bokeh](http://bokeh.pydata.org/en/latest/) library, which is similar to Plotly but much faster at rendering text; check out `plotting.py` (in this directory) if you're interested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import plotting as plot_wv\n",
    "reload(plot_wv)\n",
    "import bokeh.plotting as bp\n",
    "bp.output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the top 1000 words\n",
    "plot_wv.plot_wv(Wv, vocab, num_words=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot all the word vectors in a separate window.\n",
    "# The page might load slowly on older machines.\n",
    "plot_wv.plot_wv(Wv, vocab, num_words=vocab_size, inline=False, \n",
    "                filename=\"plots/wordvectors.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also highlight specific words by passing a color dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_colors = collections.defaultdict(lambda: \"black\")\n",
    "word_colors[\"the\"] = \"blue\"\n",
    "word_colors[\"england\"] = \"red\"\n",
    "word_colors[\"soviet\"] = \"red\"\n",
    "plot_wv.plot_wv(Wv, vocab, num_words=1000, word_colors=word_colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE\n",
    "\n",
    "The above visualization just plots the first two dimensions. This is equivalent to doing a TruncatedSVD with k=2, which doesn't always give the most meaningful representation.\n",
    "\n",
    "We can get more intuition by projecting down with [t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding). This is a *non*-linear way of embedding high-dimensional data (like our embedding vectors) into a low dimensional space. It works by preserving local distances (like nearby neighbors), at the expense of some global distortion.\n",
    "\n",
    "t-SNE won't be very good if we want to check analogy relationships, but it will help us visualize clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn includes a t-SNE implementation in [`sklearn.manifold.TSNE`](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html), but the implementation is slow and tends to crash by using too much (>4 GB) memory.\n",
    "\n",
    "Instead, we'll use the excellent [`bhtsne`](https://github.com/dominiek/python-bhtsne) package. Install with:\n",
    "```\n",
    "sudo apt-get install gcc g++\n",
    "pip install bhtsne\n",
    "```\n",
    "\n",
    "The cell below will take around 2 minutes to run on a 2 CPU Cloud Compute instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import bhtsne\n",
    "\n",
    "t0 = time.time()\n",
    "print \"Running Barnes-Hut t-SNE on word vectors; matrix shape = %s\" % str(Wv.shape)\n",
    "Wv2 = bhtsne.tsne(Wv)\n",
    "print \"Transformed in %s\" % data_utils.pretty_timedelta(since=t0)\n",
    "\n",
    "## Uncomment below if you need to use sklearn implementation\n",
    "## (not recommended)\n",
    "# from sklearn.manifold import TSNE\n",
    "# transformer = TSNE(n_components=2, verbose=2)\n",
    "# Wv2 = transformer.fit_transform(Wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_wv.plot_wv(Wv2, vocab, num_words=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_wv.plot_wv(Wv2, vocab, num_words=vocab_size, inline=False, \n",
    "                filename=\"plots/wordvectors_tsne.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration\n",
    "\n",
    "Use the cells below to experiment with word embeddings. Things to try:\n",
    "\n",
    "- Experiment with different window sizes. How do the word clusters change if you use a window of $\\pm 1$ word, versus $\\pm 3$, or full-sentence?\n",
    "- Look at a few \"target\" words of interest, using `word_colors` to highlight, or the `show_nns` function below. What are their nearest neighbors, and how does this change with the way you construct the embeddings?\n",
    "\n",
    "Feel free to modify any of the code below, or to write your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_nn_cos(word_id, Wv, k=10):\n",
    "    \"\"\"Find nearest neighbors, by cosine distance.\"\"\"\n",
    "    v = Wv[word_id]\n",
    "    Z = np.linalg.norm(Wv, axis=1) * np.linalg.norm(v)\n",
    "    ds = np.dot(Wv, v.T) / Z\n",
    "    nns = np.argsort(-1*ds)[:k]  # sort descending, take best\n",
    "    return nns, ds[nns]  # word indices, distances\n",
    "\n",
    "def show_nns(word, Wv, vocab, k=10):\n",
    "    print \"Nearest neighbors for \\\"%s\\\"\" % word\n",
    "    for i, d in zip(*find_nn_cos(vocab.word_to_id[word], Wv, k)):\n",
    "        w = vocab.id_to_word[i]\n",
    "        print \"%.03f : \\\"%s\\\"\" % (d, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Input lists\n",
    "sentence_ids = [vocab.words_to_ids(data_utils.canonicalize_words(s)) \n",
    "                for s in corpus.sents()]\n",
    "ids = list(flatten(sentence_ids))\n",
    "\n",
    "# Compute co-occurence matrix and word vectors\n",
    "C = cooccurrence_matrix(ids, vocab, window_size=1)\n",
    "# C = cooccurrence_matrix_sentences(sentence_ids, vocab)\n",
    "\n",
    "Wv, _ = SVD(PPMI(C), k=100)\n",
    "\n",
    "Wv2 = bhtsne.tsne(Wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show_nns(\"close\", Wv, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
