{
 "cells": [ 
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation Maximization\n",
    "\n",
    "Expectation maximization is very similar to k-means.\n",
    "\n",
    "Concretely, it fits its training data by repeatedly:\n",
    "1.  (E-step) Assign each data point (fractionally) to each class.\n",
    "2.  (M-step) Fit the parameters of each class's model to the data points assigned to it.\n",
    "\n",
    "The first step is called expectation because it is the expected class assignment, given the class models.\n",
    "The second step is called maximization because we adjust the parameters of our model to maximize the likelihood of the data, given the assignments.\n",
    "\n",
    "\n",
    "In addition to the async material, examples of how this is used in practice:\n",
    "- Topic modelling (partitioning documents into k classes):\n",
    "  - Assign (fractionally) each document to a topic\n",
    "  - Fit a bag of words (bernoulli or binomial) model to the documents assigned.\n",
    "\n",
    "- Gaussian mixture models (teasing apart data points generated by gaussians in various positions):\n",
    "  - Assign (fractionally) each point to a gaussian\n",
    "  - Adjust the gaussian's mean and variance to match the points assigned to it.\n",
    "  \n",
    "  \n",
    "We will explore this second case here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data from a mixture of two gaussians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zero_samples = 200\n",
    "zero_center = -3\n",
    "zero_std = 1.\n",
    "one_samples = 800\n",
    "ones_center = 2\n",
    "ones_std = 0.5\n",
    "X = np.hstack([zero_center + zero_std * np.random.randn(zero_samples),\n",
    "               ones_center + ones_std * np.random.randn(one_samples)])\n",
    "y = np.hstack([np.zeros(zero_samples), np.ones(one_samples)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEACAYAAAC9Gb03AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f78e9214190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "((1000,), (1000,))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plot_distribution(X, y):\n",
    "    bins = np.linspace(-10, 10, 100)\n",
    "    plt.hist(X[y<0.5], bins, alpha=0.4, label='-')\n",
    "    plt.hist(X[y>=0.5], bins, alpha=0.4, label='+')\n",
    "    plt.legend(loc='upper_right')\n",
    "    plt.show()\n",
    "plot_distribution(X, y)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation and Maximization Steps\n",
    "\n",
    "### Model parameters\n",
    "The model parameters we are trying to fit are:\n",
    "- mus[0], sigmas[0]:  the mean of the gaussian generating one of the two classes\n",
    "- mus[1], sigmas[1]:  the mean of the gaussian generating the other class\n",
    "- phi: the fraction of data that is in one class vs. the other.\n",
    "\n",
    "We know the answers to these (we pick them up above).  Here is one of the possible models this code will discover:\n",
    "- mus[0], sigmas[0]: zero_center, zero_std\n",
    "- mus[1], sigmas[1]: ones_center, ones_std\n",
    "- phi: one_samples/(one_samples + zero_samples)\n",
    "\n",
    "Note that the model has no idea which class was originally \"class 1\", so it's possible that mu[0] ends up modelling what we called class 1 above and vice versa.  But the set of parameters will be consistent.\n",
    "\n",
    "### Expectation\n",
    "\n",
    "This step is similar to k-means where you hold the centroids fixed and assign the data to the nearest point.  The only difference here is that we can hedge our bets a little:  each data point is assigned a little bit to each class.\n",
    "\n",
    "To compute the expected (fractional) assignment of each $X_i$, we compute its likelihood under each class' model and normalize.  We do this by applying Bayes rule:\n",
    "\n",
    "$$P(y=j | X, phi, mus, sigmas) = P(x | y=j, mus[j], sigmas[j]) * P(y=j | phi) / Z$$\n",
    "\n",
    "Note that these mus and sigmas are just numbers estimated by the previous maximization step.\n",
    "\n",
    "This leaves us the question \"What is Z?\".  As usual, we solve this by simply adding up the numerators and dividing through by that sum to normalize.\n",
    "\n",
    "### Maximization\n",
    "This step is similar to k-means where you move the centroids around, given the data points assigned to them in the E-step.\n",
    "\n",
    "While in k-means we move centroids around to improve some loss function, here we re-estimate the parameters of the gaussian generators.  Concretely, this means re-estimating mu and sigma for each class' gaussian.\n",
    "\n",
    "To do this, we assign:\n",
    "- $phi=\\frac{1}{m}\\sum w_j$\n",
    "- $mu_j = \\frac{\\sum w_j * x_i}{\\sum w_j}$\n",
    "- $sigma_j = \\frac{\\sum w_j * (x - mu_j)^2}{\\sum w_j}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gaussian(X, mu, sigma):\n",
    "    #print X, mu, sigma\n",
    "    return np.exp(-(X - mu) ** 2) / math.sqrt(2 * math.pi * sigma * sigma)\n",
    "\n",
    "def expectation(X, phi, mus, sigmas):\n",
    "    # P(y=j | X, phi, mus, sigmas) = P(x | y=j, mus, sigmas) P(y=j | phi) / Z\n",
    "    # Where Z is the normalization across all possible j's.\n",
    "    # w[i][j], at the end of this function, is the assignment (weight) of data point X_i to class j\n",
    "    w = np.zeros([len(X), len(mus)])\n",
    "    # For each data point...\n",
    "    for i in xrange(len(X)):\n",
    "        # For each class...\n",
    "        for j in xrange(len(mus)):\n",
    "            # Determine the (unnormalized) P(y=j | X, ...)\n",
    "            w[i][j] = gaussian(X[i], mus[j], sigmas[j]) * phi[j]\n",
    "        # Normalize the distribution P(y=j | X, ...) so that it sums to 1.\n",
    "        w[i] = w[i] / np.sum(w[i])\n",
    "    return w\n",
    "\n",
    "def maximization(X, w):\n",
    "    # Sum of weight of each class, useful for calculations below.\n",
    "    w_js = np.sum(w, axis=0)\n",
    "    # Estimate phi by fraction of weight assigned to each class, overall.\n",
    "    phi = w_js / np.sum(w)\n",
    "    # Estimate mus and sigmas by taking the weighted expectations of (X-mu)**2.\n",
    "    mus = np.dot(w.T, X) / w_js\n",
    "    x_minus_mu = np.tile(X, [mus.shape[0], 1]).T - mus\n",
    "    sigmas = np.sum(w * x_minus_mu**2, axis=0) / w_js\n",
    "    # Return updated estimates.\n",
    "    return phi, mus, sigmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def em(X, iterations):\n",
    "    # Pick priors on mus, sigmas and phi.\n",
    "    # Should break symmetry here, but otherwise these are arbitrary.\n",
    "    mus = np.array([0.0, 0.1])\n",
    "    sigmas = np.array([1.0, 2.0])\n",
    "    phi = np.array([0.5, 0.5])\n",
    "\n",
    "    for i in xrange(iterations):\n",
    "        print 'phi:', phi\n",
    "        print 'mus:', mus\n",
    "        print 'sigmas:', np.sqrt(sigmas)\n",
    "        print '-'\n",
    "        w = expectation(X, phi, mus, sigmas)\n",
    "        phi, mus, sigmas = maximization(X, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment!\n",
    "\n",
    "With that code out of the way, let's see how well we can recover the distributions generating our data.\n",
    "\n",
    "You can see it converges (very) quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phi: [ 0.5  0.5]\n",
      "mus: [ 0.   0.1]\n",
      "sigmas: [ 1.          1.41421356]\n",
      "-\n",
      "phi: [ 0.61653859  0.38346141]\n",
      "mus: [ 0.71550178  1.48451346]\n",
      "sigmas: [ 2.26667182  1.63776527]\n",
      "-\n",
      "phi: [ 0.3569243  0.6430757]\n",
      "mus: [-0.89052157  2.06544406]\n",
      "sigmas: [ 2.46177969  0.51014505]\n",
      "-\n",
      "phi: [ 0.19914183  0.80085817]\n",
      "mus: [-2.97485282  2.00136027]\n",
      "sigmas: [ 0.96222455  0.50268568]\n",
      "-\n",
      "phi: [ 0.19745206  0.80254794]\n",
      "mus: [-2.99662707  1.99623998]\n",
      "sigmas: [ 0.93550214  0.51503928]\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "em(X, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
